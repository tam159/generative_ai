{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376c865a-9f50-417d-a0cf-4c430571d104",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "This notebook covers basic walkthrough of retrieval functionality in LangChain. For more information, see:\n",
    "\n",
    "- [Retrieval Documentation](https://python.langchain.com/docs/modules/data_connection/)\n",
    "\n",
    "- [Advanced Retrieval Types](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "- [QA with RAG Use Case Documentation](https://python.langchain.com/docs/use_cases/question_answering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b594de",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:50:59.292449Z",
     "start_time": "2024-01-14T15:50:59.225061Z"
    }
   },
   "id": "6b9825876dd13b38",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T15:51:08.207098Z",
     "start_time": "2024-01-14T15:51:08.200614Z"
    }
   },
   "id": "dd7201397242c02b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bda46c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:51:13.474303Z",
     "start_time": "2024-01-14T15:51:12.471795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb65747c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:51:15.218761Z",
     "start_time": "2024-01-14T15:51:15.208330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aece40e",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e749d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:51:19.259244Z",
     "start_time": "2024-01-14T15:51:19.252984Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006a257f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:51:43.659578Z",
     "start_time": "2024-01-14T15:51:43.647870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792485da",
   "metadata": {},
   "source": [
    "## Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289e53d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:52:05.136429Z",
     "start_time": "2024-01-14T15:51:56.473900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d2450",
   "metadata": {},
   "source": [
    "## Query Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a66449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:52:54.259547Z",
     "start_time": "2024-01-14T15:52:53.777922Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588f5818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:53:57.080103Z",
     "start_time": "2024-01-14T15:53:57.074352Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93cfd3ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:54:05.603712Z",
     "start_time": "2024-01-14T15:53:59.968458Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing the following features:\n",
      "\n",
      "1. Dataset Uploading: LangSmith simplifies the process of uploading datasets for testing changes to a prompt or chain.\n",
      "\n",
      "2. Running Chains and Agents: LangSmith allows running chains over data points and visualizing the outputs. This can be done client-side using the LangSmith client.\n",
      "\n",
      "3. Logging Results: When running chains over data points, LangSmith logs the results to a new project associated with the dataset, making it easy to review them.\n",
      "\n",
      "4. Assigning Feedback: LangSmith enables assigning feedback to runs and marking them as correct or incorrect directly in the web app. This helps in reviewing and analyzing the results.\n",
      "\n",
      "5. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators are not perfect, they can guide the user to examples that require further attention.\n",
      "\n",
      "6. Human Evaluation: LangSmith makes it easy to manually review and annotate runs through annotation queues. Reviewers can assess subjective qualities, validate auto-evaluated runs, and provide feedback. All annotations made using the queue are assigned as \"feedback\" to the source runs for later analysis.\n",
      "\n",
      "Overall, LangSmith provides tools and features that facilitate testing and evaluation of LLM applications, ensuring the utmost quality and reliability.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ba679",
   "metadata": {},
   "source": [
    "## Advanced Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d89b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:55:24.041273Z",
     "start_time": "2024-01-14T15:55:23.893783Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67347a0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:55:24.923608Z",
     "start_time": "2024-01-14T15:55:24.917661Z"
    }
   },
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685ccfb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T15:55:37.777467Z",
     "start_time": "2024-01-14T15:55:29.960550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the benefits of using Langsmith for testing purposes?', '2. In what ways does Langsmith assist with testing?', '3. Can you explain how Langsmith contributes to the testing process?']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing various features and functionalities. It allows users to:\n",
      "\n",
      "1. Easily curate datasets: LangSmith makes it easy to create and manage datasets that can be used for testing and evaluation purposes. These datasets can be exported for use in other contexts.\n",
      "\n",
      "2. Run chains over data points: Users can run chains over data points and visualize the outputs. This helps in testing and analyzing the chain's behavior and performance.\n",
      "\n",
      "3. Assign feedback programmatically: LangSmith allows users to associate feedback programmatically with runs. This is useful for tracking performance over time and identifying underperforming data points.\n",
      "\n",
      "4. Extract insights from logged runs: LangSmith provides examples and guidance on extracting insights from logged runs. It helps in understanding the sequence of events, identifying the exact input and output of LLM calls, and troubleshooting specific issues.\n",
      "\n",
      "5. Evaluate runs with evaluators: LangSmith offers a set of evaluators that can be specified during a test run. These evaluators help in evaluating the results and highlighting examples that require further attention.\n",
      "\n",
      "6. Enable human evaluation: While automatic evaluation metrics are helpful, LangSmith also facilitates human review and annotation of runs. Users can manually review and annotate runs through annotation queues, allowing for subjective assessments and validation of automatic metrics.\n",
      "\n",
      "Overall, LangSmith simplifies the testing process by providing tools and features to track, evaluate, and analyze the performance of LLM applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e992b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
