{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do function calling with the Converse API \n",
    "\n",
    "This notebook demonstrates how we can improve model capability by using the Converse or ConverseStream API with external functions.\n",
    "\n",
    "Converse and ConverseStream provide a unified structured text action for simplifying the invocations to Bedrock LLMs. It includes the possibility to define tools for implementing external functions that can be called or triggered from the LLMs.\n",
    "\n",
    "As part of these APIs, the `toolConfig` parameter takes a list of `tools` in JSON schema. Each tool is defined as a `ToolSpec`, which includes its name, description and input schema. The model can be forced to call a given tool by supplying `toolChoice` within `toolConfig` with `{\"tool\": {\"name\": <some_tool_name>}}`. Each tool receives a unique identifier.\n",
    "\n",
    "When the model chooses to call a tool, the response contains a `toolUse` object. It includes the tool's identifier, name and the input parameters supplied by the model. The model will also set the `stopReason` response field to `tool_use`.\n",
    "\n",
    "## Overview\n",
    "- **Basic tool definition and function calling** We define a single tool for simulating a weather forecast lookup tool (`get_weather`) and allow the model to call this tool.\n",
    "- **Supplying the model with multiple tools to choose from** Starting from the previous example, we add a tool to search the web (`web_search`) and give the the model the liberty to decide the tool that is best fit for a given request.\n",
    "- **Letting the model call multiple tools in a single turn** We modify the conversation flow to support parallel function calling. Then, we present an example where the model needs to call multiple tools in succession.\n",
    "\n",
    "If you want to learn more details about the parameters supported in the Bedrock Converse API, read the [api reference](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) and [user guide](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n",
    "\n",
    "## Basic tool definition and function-calling\n",
    "\n",
    "We set our tools through Python functions and start by defining a tool for simulating a weather forecast lookup tool (`get_weather`). Note in our example we're just returning a constant weather forecast to illustrate the concept, but you could make it fully functional by connecting any weather service API. Later in the example, we call the Open-Meteo API."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:10.179817Z",
     "start_time": "2025-05-28T12:40:10.177222Z"
    }
   },
   "source": [
    "# !pip3 install boto3 --quiet\n",
    "# !pip3 install googlesearch-python --quiet\n",
    "# !pip3 install lxml --quiet\n",
    "# !pip3 install pydantic --quiet"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this example leverages Claude 3 Sonnet, Bedrock supports many other models. The full list of models and supported features can be found [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html). The models are invoked via `bedrock-runtime`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:47.019060Z",
     "start_time": "2025-05-28T12:40:46.986702Z"
    }
   },
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "import inspect\n",
    "import boto3\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "modelId = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-east-1'\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool definition \n",
    "We define `ToolsList` as a class where individual tools are defined as functions. Note that there is nothing specific to the model used or Bedrock in this definition."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:49.000339Z",
     "start_time": "2025-05-28T12:40:48.994722Z"
    }
   },
   "source": [
    "class ToolsList:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        print(f'Tool result: {result}')\n",
    "        return result"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within `toolConfig`, setting `toolChoice` to `{auto: {}}` gives the model the choice to decide if a tool should be called or if it can rely on its internal body of knowledge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:50.025505Z",
     "start_time": "2025-05-28T12:40:50.020554Z"
    }
   },
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {\n",
    "    'tools': [],\n",
    "    'toolChoice': {\n",
    "        'auto': {}\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's structure our tool configuration and append it to our `tools`. We have to clearly define the schema that our tools are expecting in the corresponding functions. The descriptions we provide allow the model to get a sense of the external function's purpose. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:50.616247Z",
     "start_time": "2025-05-28T12:40:50.610664Z"
    }
   },
   "source": [
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply calls the Converse API given some `toolConfig` and returns the response"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:51.262992Z",
     "start_time": "2025-05-28T12:40:51.256373Z"
    }
   },
   "source": [
    "#Function for caling the Bedrock Converse API...\n",
    "def converse_with_tools(messages, system='', toolConfig=toolConfig):\n",
    "    response = bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the conversation flow\n",
    "\n",
    "Next, we define a function to manage the conversation flow. For this simple case, the function starts by invoking the model. Should the model choose to execute the tool we have defined, it returns it in `toolUse`. With this, the function runs the selected tool. Lastly, the tool's output is returned in `toolResults` to the model who can be given instructions to format it in a more conversational tone for the user. \n",
    "\n",
    "#### Prompt flow\n",
    "![basic tool call](./assets/basic_tool_call.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:51.994623Z",
     "start_time": "2025-05-28T12:40:51.988891Z"
    }
   },
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "        if tool_response:\n",
    "            tool_status = 'success'\n",
    "        else:\n",
    "            tool_status = 'error'\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ],\n",
    "                            'status': tool_status\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything setup and are ready for testing our function calling bot.\n",
    "\n",
    "If we prompt our model about the weather, it will respond with the formatted string defined in our tool."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:41:00.900641Z",
     "start_time": "2025-05-28T12:40:52.854700Z"
    }
   },
   "source": [
    "prompt = \"What is the weather like in Queens, NY?\"\n",
    "\n",
    "\n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather'; \\\n",
    "        only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "    prompt = prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather like in Queens, NY?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"To get the weather information for Queens, NY, I'll need to use the get_weather function. Let me fetch that information for you.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_96T2Cj3rQAqq1OGqe_jhGg\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Queens\",\n",
      "            \"state\": \"NY\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Running (get_weather) tool...\n",
      "Tool result: Weather in Queens, NY is 70F and clear skies.\n",
      "Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Based on the information I've received, the weather in Queens, NY is currently 70째F (about 21째C) with clear skies. This is quite pleasant weather, ideal for outdoor activities or simply enjoying a nice day. The clear skies suggest good visibility and no immediate threat of rain. It's a great temperature for walking, cycling, or spending time in parks without feeling too hot or cold. Remember to stay hydrated and consider applying sunscreen if you plan to spend extended time outdoors, especially during the middle of the day when the sun is strongest.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask a question about another topic, the model will answer the question directly without making a tool call. Notice the absence of `toolUse` in the output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:41:24.919990Z",
     "start_time": "2025-05-28T12:41:21.065590Z"
    }
   },
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather'; \\\n",
    "        only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "    prompt = prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the capital of France?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"I apologize, but I don't have a specific tool available to directly provide information about the capital of France. However, I can answer your question based on general knowledge:\\n\\nThe capital of France is Paris. \\n\\nParis is not only the capital city but also the largest city in France. It's known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. Paris is also a global center for art, fashion, gastronomy, and culture.\\n\\nIs there anything specific about Paris or France that you'd like to know more about?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether or not to call the `get_weather` tool depending on the question. You can further improve this example by playing with the system prompts.\n",
    "\n",
    "## Supplying the model with multiple tools to choose from\n",
    "\n",
    "In many cases, it makes sense to supply the model with multiple external functions to choose from. We explore a slightly more evolved case from the one described above to not only have a `get_weather` tool, but also have a fully functional `web_search` tool that can look up information in the Internet for enriching our responses. For this, we'll leverage a simple public library with a simple web scrapping implementation.\n",
    "\n",
    "### Tool definition\n",
    "\n",
    "We again define `ToolsList` as a class where individual tools are defined as functions.\n",
    "\n",
    "![basic tool call](./assets/function_calling_multiple.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:44:38.053947Z",
     "start_time": "2025-05-28T12:44:37.803697Z"
    }
   },
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ToolsList:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        #print(city, state)\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        return result\n",
    "\n",
    "    # Define our web_search tool function...\n",
    "    def web_search(self, query):\n",
    "        results = []\n",
    "        response_list = []\n",
    "        results.extend([r for r in search(query, 3, 'en')])\n",
    "        for j in results:\n",
    "            response = requests.get(j)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                response_list.append(soup.get_text().strip())\n",
    "        response_text = \",\".join(str(i) for i in response_list)\n",
    "        return response_text"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset `toolConfig` to load it with the new set of tools."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:44:40.934932Z",
     "start_time": "2025-05-28T12:44:40.929518Z"
    }
   },
   "source": [
    "toolConfig = {'tools': [], 'toolChoice': {'auto': {}}}"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, we append the new tools to the `tools` section of the object."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:44:43.517239Z",
     "start_time": "2025-05-28T12:44:43.511926Z"
    }
   },
   "source": [
    "# Add the get_weather tool...\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Add the web_search tool...\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'web_search',\n",
    "            'description': 'Search a term in the public Internet. \\\n",
    "                Useful for getting up to date information.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'search_term': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'Term to search in the Internet'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['search_term']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the conversation flow\n",
    "\n",
    "We reuse the `converse` method defined above to control the flow of conversation. It can be reused to support any number of tools. \n",
    "\n",
    "If we prompt our model about the weather, it will respond with the same formatted string defined in our `get_weather` tool."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:44:53.607531Z",
     "start_time": "2025-05-28T12:44:46.712449Z"
    }
   },
   "source": [
    "prompt = \"What is the weather like in Queens, NY?\"\n",
    "    \n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather', and another tool to perform a web search for up to date information 'web_search'; \\\n",
    "        use those tools if required. Don't mention the tools in your final answer.\"}],\n",
    "    prompt = prompt\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather like in Queens, NY?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"To get the weather information for Queens, NY, I'll need to use the get_weather function. Let me fetch that information for you.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_Z0lM40cKRtWGRXeavaFlLQ\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Queens\",\n",
      "            \"state\": \"NY\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Running (get_weather) tool...\n",
      "Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Based on the information I've retrieved, the weather in Queens, NY is currently 70째F (about 21째C) with clear skies. This sounds like a pleasant day with comfortable temperatures and no cloud cover. It's a great day to spend some time outdoors if you have the opportunity. Remember to dress appropriately for the temperature and perhaps bring sunglasses or sunscreen if you plan to be outside for an extended period, as clear skies can mean more direct sunlight.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we prompt our model with another topic, it will respond a result from the web by using the `web_search` tool."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:45:20.645059Z",
     "start_time": "2025-05-28T12:45:02.829512Z"
    }
   },
   "source": [
    "prompt = \"In which team is Caitlin Clark playing in the WNBA in 2024?\"\n",
    "\n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather', and another tool to perform a web search for up to date information 'web_search'; \\\n",
    "        use those tools if required. Don't mention the tools in your final answer.\"}],\n",
    "    prompt = prompt\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"In which team is Caitlin Clark playing in the WNBA in 2024?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"To answer this question accurately, I'll need to search for up-to-date information about Caitlin Clark's WNBA status in 2024. Let me do that for you.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_GDdYbXZgSBqa-xFkkmgyww\",\n",
      "          \"name\": \"web_search\",\n",
      "          \"input\": {\n",
      "            \"query\": \"Caitlin Clark WNBA team 2024\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Running (web_search) tool...\n",
      "Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Based on the information provided, here are the key details about Caitlin Clark's first WNBA season with the Indiana Fever in 2024:\\n\\n- Clark was selected 1st overall by the Indiana Fever in the 2024 WNBA draft.\\n\\n- She played 40 games in her rookie season, starting all 40.\\n\\n- Clark averaged 19.2 points, 5.7 rebounds, and 8.4 assists per game. \\n\\n- Her 8.4 assists per game led the WNBA.\\n\\n- She shot 41.7% from the field, 34.4% from three-point range, and 90.6% from the free throw line.\\n\\n- Clark was named WNBA Rookie of the Year.\\n\\n- She made the All-WNBA First Team, becoming the first rookie to do so since Candace Parker in 2008.\\n\\n- Clark was selected as an All-Star in her first season.\\n\\n- She set new WNBA rookie records for assists and three-pointers made in a season.\\n\\n- The Fever improved to a 20-20 record and made the playoffs for the first time since 2016 with Clark leading the team.\\n\\nSo in summary, Caitlin Clark had a historically great rookie season, leading the league in assists while also scoring at a high level and helping turn around the Fever franchise. She lived up to the hype as the #1 overall pick and established herself as one of the WNBA's top players immediately.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether to call the `get_weather` tool, provide an answer without any tool, or searching in the public Internet with the `web_search` tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letting the model call multiple tools in a single turn\n",
    "\n",
    "### Parallel functions\n",
    "Parallel function refers to the model's ability to make multiple calls to one or more tools in a single turn. By allowing the model to decompose the requester's question into multiple subproblems each solvable with a tool, it is able to increase its level flexibility.\n",
    "\n",
    "#### Tool as Pydantic definition\n",
    "To do this, we use Pydantic, a data-validation library. We rely on a Pydantic-based helper function for doing the tool config translation for us in a way that ensures we avoid potential mistakes when defining our tool config schema in a JSON dictionary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:28.052314Z",
     "start_time": "2025-05-28T12:49:28.040967Z"
    }
   },
   "source": [
    "def tool(name, description):\n",
    "    def decorator(func):\n",
    "        # defining our model inheriting from pydantic.BaseModel and define fields as annotated attributes\n",
    "        input_model = create_model(\n",
    "            func.__name__ + \"_input\",\n",
    "            **{\n",
    "                name: (param.annotation, param.default)\n",
    "                for name, param in inspect.signature(func).parameters.items()\n",
    "                if param.default is not inspect.Parameter.empty\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # bedrock tool schema\n",
    "        func.bedrock_schema = {\n",
    "            'toolSpec': {\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'inputSchema': {\n",
    "                    'json': input_model.model_json_schema()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    return decorator"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again define our tool list, and all we need to specify is the name, description, any relevant attributes that are required in our function. Of course we should also add the content of our tool. We define the `get_lat_long` tool to call Open Streat Map to retrieve these coordinates.\n",
    "\n",
    "If you have more tools, you can adapt this cell for testing with your own."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:29.441293Z",
     "start_time": "2025-05-28T12:49:29.434933Z"
    }
   },
   "source": [
    "# Define your tools\n",
    "class ToolsList:\n",
    "    # define get_lat_long tool\n",
    "    @tool(\n",
    "        name=\"get_lat_long\",\n",
    "        description=\"Get the coordinates of a city based on a location.\"\n",
    "    )\n",
    "    def get_lat_long(self, place: str = Field(..., description=\"City of the location\")) -> dict:\n",
    "        \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "        header_dict = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "            \"referer\": 'https://www.guichevirtual.com.br'\n",
    "        }\n",
    "        url = \"http://nominatim.openstreetmap.org/search\"\n",
    "        params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "        response = requests.get(url, params=params, headers=header_dict).json()\n",
    "        if response:\n",
    "            lat = response[0][\"lat\"]\n",
    "            lon = response[0][\"lon\"]\n",
    "            return {\"latitude\": lat, \"longitude\": lon}\n",
    "        else:\n",
    "            return None"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our configuration, setup the function for invoking Bedrock with Converse, and define our workflow orchestration function as per the previous examples. Using the Pydantic definition allows us to generalize `toolConfig` to any tool supplied to `ToolsList`. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:37.691045Z",
     "start_time": "2025-05-28T12:49:37.686774Z"
    }
   },
   "source": [
    "toolConfig = {\n",
    "    'tools': [tool.bedrock_schema for tool in ToolsList.__dict__.values() if hasattr(tool, 'bedrock_schema')],\n",
    "    'toolChoice': {'auto': {}}\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the conversation flow\n",
    "\n",
    "The following `converse` method has been adapted from the previous one to support parallel function calling in a single turn of the conversation. As it did before, the conversation begins with an invocation of the model dedicing whether to use a tool or not. During this step, the model can choose to call any number of tool as many times as it is necessary to complete the request. The model is then supplied the result of the executions in `toolResult` during the subsequent invokation.\n",
    "\n",
    "![basic tool call](./assets/parallel_function_call.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:43.552836Z",
     "start_time": "2025-05-28T12:49:43.544897Z"
    }
   },
   "source": [
    "def converse(tool_class, prompt, system='', toolConfig=None, modelId=modelId):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    print(\"Invoking model...\")\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    messages.append(output['output']['message'])\n",
    "    print(\"Got output from model...\")\n",
    "\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        tool_result_message = {\"role\": \"user\", \"content\": []}\n",
    "        for function in function_calling:\n",
    "            print(\"Function calling - Calling tool...\")\n",
    "            tool_name = function['name']\n",
    "            tool_args = function['input'] or {}\n",
    "            tool_response = json.dumps(getattr(tool_class, tool_name)(**tool_args))\n",
    "            print(\"Function calling - Got tool response...\")\n",
    "            tool_result_message['content'].append({\n",
    "                'toolResult': {\n",
    "                    'toolUseId': function['toolUseId'],\n",
    "                    'content': [{\"text\": tool_response}]\n",
    "                }\n",
    "            })\n",
    "        messages.append(tool_result_message)\n",
    "        print(\"Function calling - Calling model with result...\")\n",
    "        \n",
    "        output = converse_with_tools(messages, system, toolConfig)\n",
    "    return messages, output"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a system prompt describing the model's role and task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:45.830843Z",
     "start_time": "2025-05-28T12:49:45.825241Z"
    }
   },
   "source": [
    "# System prompt building an optional chain of thought response\n",
    "system_prompt = \"\"\"You're provided with a tool that can get the coordinates for a specific city 'get_lat_long';\n",
    "    only use the tool if required. You can call the tool multiple times in the same response if required. \\\n",
    "    Don't make reference to the tools in your final answer.\"\"\"\n",
    "system_prompt = [{\"text\": system_prompt}]"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask for the coordinates of two cities, the model calls `get_lat_long` twice and aggregates the model answers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:49:57.291040Z",
     "start_time": "2025-05-28T12:49:46.909696Z"
    }
   },
   "source": [
    "# prompt to get up-to-date coordinates of Montreal and Toronto\n",
    "prompt = \"What are the coordinates for both Paris and in Berlin??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling model with result...\n",
      "{'ResponseMetadata': {'RequestId': '24355ef4-0f60-4b02-a765-3a67f4f0c3b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 May 2025 12:49:57 GMT', 'content-type': 'application/json', 'content-length': '678', 'connection': 'keep-alive', 'x-amzn-requestid': '24355ef4-0f60-4b02-a765-3a67f4f0c3b0'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': \"Great! I've retrieved the coordinates for both Paris and Berlin. Here they are:\\n\\n1. Paris, France:\\n   Latitude: 48.8588897\\n   Longitude: 2.3200410\\n\\n2. Berlin, Germany:\\n   Latitude: 52.5108850\\n   Longitude: 13.3989367\\n\\nThese coordinates represent the approximate center of each city. Paris is located in the northern part of France, while Berlin is in the northeastern part of Germany. The coordinates are given in decimal degrees, which is a common format for geographic coordinates.\"}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 703, 'outputTokens': 127, 'totalTokens': 830}, 'metrics': {'latencyMs': 3561}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': \"Great! I've retrieved the coordinates for both Paris and Berlin. Here they are:\\n\\n1. Paris, France:\\n   Latitude: 48.8588897\\n   Longitude: 2.3200410\\n\\n2. Berlin, Germany:\\n   Latitude: 52.5108850\\n   Longitude: 13.3989367\\n\\nThese coordinates represent the approximate center of each city. Paris is located in the northern part of France, while Berlin is in the northeastern part of Germany. The coordinates are given in decimal degrees, which is a common format for geographic coordinates.\"}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What are the coordinates for both Paris and in Berlin??\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Certainly! I can help you find the coordinates for both Paris and Berlin. To get this information, I'll need to use the available tool twice - once for each city. Let me fetch that data for you.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_re2ky-jQRL6XKq85gULJjw\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Paris\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_-gXIEiNPSaiTgxPCck5pww\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Berlin\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_re2ky-jQRL6XKq85gULJjw\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"48.8588897\\\", \\\"longitude\\\": \\\"2.3200410\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_-gXIEiNPSaiTgxPCck5pww\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"52.5108850\\\", \\\"longitude\\\": \\\"13.3989367\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool chaining refers to the ability of the model to reach its goal by calling more than one tool where the output of one tool serves as the input to the next. This more complex workflow requires the model to breakdown the query into subproblems that can individually be met with a call to an external function or the model's own body of knowledge.\n",
    "\n",
    "We define `ToolList` again to include `get_lat_long` and a modified version of the previous `get_weather` function. This new version lerverages the Open-Meteo service translating a given set of coordinates to the currrent weather at those coordinates. Logically, this means that for a given question about the weather in a location, the model must first retrieve the coordinates of that place using `get_lat_long` and use those coordinates when calling `get_weather`. These calls occur in two seperate steps where the model is able to observe outputs, which differs from parallel functions call where the model selects multiple tools in a single turn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:52:56.322745Z",
     "start_time": "2025-05-28T12:52:56.303352Z"
    }
   },
   "source": [
    "# Define your tools\n",
    "class ToolsList:\n",
    "    # define get_lat_long tool\n",
    "    @tool(\n",
    "        name=\"get_lat_long\",\n",
    "        description=\"Get the coordinates of a city based on a location.\"\n",
    "    )\n",
    "    def get_lat_long(self, place: str = Field(..., description=\"City of the location\")) -> dict:\n",
    "        \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "        header_dict = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "            \"referer\": 'https://www.guichevirtual.com.br'\n",
    "        }\n",
    "        url = \"http://nominatim.openstreetmap.org/search\"\n",
    "        params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "        response = requests.get(url, params=params, headers=header_dict).json()\n",
    "        if response:\n",
    "            lat = response[0][\"lat\"]\n",
    "            lon = response[0][\"lon\"]\n",
    "            return {\"latitude\": lat, \"longitude\": lon}\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    # define get_weather tool...\n",
    "    @tool(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Get weather of a location.\"\n",
    "    )\n",
    "    def get_weather(self,\n",
    "        latitude: str = Field(..., description=\"latitude of the location\"), \n",
    "        longitude: str = Field(..., description=\"longitude of the location\")) -> dict:\n",
    "        \"\"\"Returns weather data for a given latitude and longitude.\"\"\"\n",
    "        url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "        response = requests.get(url)\n",
    "        return response.json()"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset `toolConfig` with the new set of functions in `ToolList`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:52:58.509320Z",
     "start_time": "2025-05-28T12:52:58.503076Z"
    }
   },
   "source": [
    "toolConfig = {\n",
    "    'tools': [tool.bedrock_schema for tool in ToolsList.__dict__.values() if hasattr(tool, 'bedrock_schema')],\n",
    "    'toolChoice': {'auto': {}}\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the conversation flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the `converse` function, we create a helper function to wrap the call of the function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:53:04.571016Z",
     "start_time": "2025-05-28T12:53:04.565671Z"
    }
   },
   "source": [
    "def call_function_and_add_message(tool_class, messages, previous_output):\n",
    "    function_calling = [c['toolUse'] for c in previous_output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        function = function_calling[0]\n",
    "        tool_result_message = {\"role\": \"user\", \"content\": []}\n",
    "        print(\"Function calling - Calling tool...\")\n",
    "        tool_name = function['name']\n",
    "        tool_args = function['input'] or {}\n",
    "        tool_response = json.dumps(getattr(tool_class, tool_name)(**tool_args))\n",
    "        print(\"Function calling - Got tool response...\")\n",
    "        tool_result_message['content'].append({\n",
    "            'toolResult': {\n",
    "                'toolUseId': function['toolUseId'],\n",
    "                'content': [{\"text\": tool_response}]\n",
    "            }\n",
    "        })\n",
    "        messages.append(tool_result_message)\n",
    "    return messages"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously described, our workflow involves at most two function calls. We adjust the `converse` function again to match this flow.\n",
    "\n",
    "![basic tool call](./assets/function_call_succession.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:53:22.747542Z",
     "start_time": "2025-05-28T12:53:22.739259Z"
    }
   },
   "source": [
    "def converse(tool_class, prompt, system='', toolConfig=None, modelId=modelId):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    \n",
    "    print(\"Invoking model...\")\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        messages.append(output['output']['message'])\n",
    "    \n",
    "    print(\"Got output from model...\")\n",
    "    messages = call_function_and_add_message(tool_class, messages, output)\n",
    "    \n",
    "    print(\"Function calling - Calling model with result...\")\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        messages.append(output['output']['message'])\n",
    "    \n",
    "    print(\"Got output from model...\")\n",
    "    messages = call_function_and_add_message(tool_class, messages, output)\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    return messages, output"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust the system prompt to account for the availability of multiple tools. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:53:24.230321Z",
     "start_time": "2025-05-28T12:53:24.222952Z"
    }
   },
   "source": [
    "# System prompt building an optional chain of thought response\n",
    "system_prompt = \"\"\"You're provided with a tool that can get the coordinates for a specific city 'get_lat_long'\n",
    "    and a tool that can get the weather for that city, but requires the coordinates 'get_weather';\n",
    "    only use the tool if required. You can call the tool multiple times in the same response if required. \\\n",
    "    Don't make reference to the tools in your final answer.\"\"\"\n",
    "system_prompt = [{\"text\": system_prompt}]"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask for the weather of a given city, the model calls `get_lat_long` then `get_weather`. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:53:39.369168Z",
     "start_time": "2025-05-28T12:53:26.722013Z"
    }
   },
   "source": [
    "# prompt to get up-to-date weather on Montreal\n",
    "prompt = \"What is the weather in Montreal??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling model with result...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "{'ResponseMetadata': {'RequestId': '46b8da8c-dd9a-4494-a739-c76c3b4e1794', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 May 2025 12:53:39 GMT', 'content-type': 'application/json', 'content-length': '700', 'connection': 'keep-alive', 'x-amzn-requestid': '46b8da8c-dd9a-4494-a739-c76c3b4e1794'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': \"Based on the information I've received, here's the current weather in Montreal:\\n\\nTemperature: 17.2째C (about 63째F)\\nWind Speed: 4.1 km/h (about 2.5 mph)\\nWind Direction: 218째 (southwest)\\nWeather Condition: Clear sky (based on the weather code 0)\\n\\nIt's currently daytime in Montreal, and the weather seems quite pleasant with clear skies and mild temperatures. It's a good day to be out and about in the city!\\n\\nIs there anything else you'd like to know about the weather in Montreal or any other location?\"}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 1042, 'outputTokens': 140, 'totalTokens': 1182}, 'metrics': {'latencyMs': 4442}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': \"Based on the information I've received, here's the current weather in Montreal:\\n\\nTemperature: 17.2째C (about 63째F)\\nWind Speed: 4.1 km/h (about 2.5 mph)\\nWind Direction: 218째 (southwest)\\nWeather Condition: Clear sky (based on the weather code 0)\\n\\nIt's currently daytime in Montreal, and the weather seems quite pleasant with clear skies and mild temperatures. It's a good day to be out and about in the city!\\n\\nIs there anything else you'd like to know about the weather in Montreal or any other location?\"}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather in Montreal??\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Certainly! I'd be happy to check the weather in Montreal for you. To do that, I'll need to first get the coordinates of Montreal and then use those to fetch the current weather information. Let me do that for you now.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_46wRKtGKQW6sB35JnFnG0Q\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Montreal\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_46wRKtGKQW6sB35JnFnG0Q\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"45.5031824\\\", \\\"longitude\\\": \\\"-73.5698065\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Great, I've got the coordinates for Montreal. Now, let me use these to fetch the current weather information.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_ZDEBRaDHSUC-Q7kSJBcKEw\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"latitude\": \"45.5031824\",\n",
      "            \"longitude\": \"-73.5698065\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_ZDEBRaDHSUC-Q7kSJBcKEw\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": 45.49215, \\\"longitude\\\": -73.56103, \\\"generationtime_ms\\\": 0.047326087951660156, \\\"utc_offset_seconds\\\": 0, \\\"timezone\\\": \\\"GMT\\\", \\\"timezone_abbreviation\\\": \\\"GMT\\\", \\\"elevation\\\": 51.0, \\\"current_weather_units\\\": {\\\"time\\\": \\\"iso8601\\\", \\\"interval\\\": \\\"seconds\\\", \\\"temperature\\\": \\\"\\\\u00b0C\\\", \\\"windspeed\\\": \\\"km/h\\\", \\\"winddirection\\\": \\\"\\\\u00b0\\\", \\\"is_day\\\": \\\"\\\", \\\"weathercode\\\": \\\"wmo code\\\"}, \\\"current_weather\\\": {\\\"time\\\": \\\"2025-05-28T12:45\\\", \\\"interval\\\": 900, \\\"temperature\\\": 17.2, \\\"windspeed\\\": 4.1, \\\"winddirection\\\": 218, \\\"is_day\\\": 1, \\\"weathercode\\\": 0}}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask for the coordinates of a given city, the model calls `get_lat_long`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:54:29.539068Z",
     "start_time": "2025-05-28T12:54:20.623640Z"
    }
   },
   "source": [
    "# prompt to get up-to-date weather on Montreal\n",
    "prompt = \"What are the coordinates in Montreal??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling model with result...\n",
      "Got output from model...\n",
      "{'ResponseMetadata': {'RequestId': 'beba413b-a769-4862-bcf1-641347aab237', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 May 2025 12:54:29 GMT', 'content-type': 'application/json', 'content-length': '464', 'connection': 'keep-alive', 'x-amzn-requestid': 'beba413b-a769-4862-bcf1-641347aab237'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': 'Based on the results, the coordinates for Montreal are:\\n\\nLatitude: 45.5031824\\nLongitude: -73.5698065\\n\\nThese coordinates represent the approximate center of Montreal, Canada. The city is located in the province of Quebec, on the island of Montreal in the Saint Lawrence River.'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 711, 'outputTokens': 68, 'totalTokens': 779}, 'metrics': {'latencyMs': 2434}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': 'Based on the results, the coordinates for Montreal are:\\n\\nLatitude: 45.5031824\\nLongitude: -73.5698065\\n\\nThese coordinates represent the approximate center of Montreal, Canada. The city is located in the province of Quebec, on the island of Montreal in the Saint Lawrence River.'}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What are the coordinates in Montreal??\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Certainly! I can help you find the coordinates for Montreal. To get this information, I'll use the get_lat_long function to retrieve the latitude and longitude of Montreal.\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_PnUZ702cSRGugB-KLBoQbw\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Montreal\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_PnUZ702cSRGugB-KLBoQbw\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"45.5031824\\\", \\\"longitude\\\": \\\"-73.5698065\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask an unrelated question to our tools, the model will not call the tools."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:54:52.205975Z",
     "start_time": "2025-05-28T12:54:32.480388Z"
    }
   },
   "source": [
    "# prompt to get up-to-date weather on Montreal\n",
    "prompt = \"What is SageMaker??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling model with result...\n",
      "Got output from model...\n",
      "{'ResponseMetadata': {'RequestId': '0af5af7e-6c2f-4327-8bac-bb3647749aac', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 May 2025 12:54:52 GMT', 'content-type': 'application/json', 'content-length': '1022', 'connection': 'keep-alive', 'x-amzn-requestid': '0af5af7e-6c2f-4327-8bac-bb3647749aac'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': \"I apologize, but I don't have specific information about SageMaker in my knowledge base, and there are no tools available to me to look up information about it. SageMaker is likely a product or service, but I can't provide accurate details without more context or the ability to search for current information.\\n\\nTo give you a proper answer, I would need to research this topic. However, I don't have the capability to do web searches or access external databases. \\n\\nIf you have more specific questions about SageMaker or if you can provide some context about where you've heard of it or what you think it might be related to, I'd be happy to try to assist you further based on that information. Alternatively, you might want to check official sources or documentation for the most up-to-date and accurate information about SageMaker.\"}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 587, 'outputTokens': 183, 'totalTokens': 770}, 'metrics': {'latencyMs': 6052}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': \"I apologize, but I don't have specific information about SageMaker in my knowledge base, and there are no tools available to me to look up information about it. SageMaker is likely a product or service, but I can't provide accurate details without more context or the ability to search for current information.\\n\\nTo give you a proper answer, I would need to research this topic. However, I don't have the capability to do web searches or access external databases. \\n\\nIf you have more specific questions about SageMaker or if you can provide some context about where you've heard of it or what you think it might be related to, I'd be happy to try to assist you further based on that information. Alternatively, you might want to check official sources or documentation for the most up-to-date and accurate information about SageMaker.\"}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is SageMaker??\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "This notebook demonstrates function calling with the Converse API with one or multiple tools.These tools can be called both in parallel and in succession. As a next step, explore similar implementation using the **Langchain** integration with Amazon Bedrock's Converse API to reduce the amount of code necessary for the solution."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
