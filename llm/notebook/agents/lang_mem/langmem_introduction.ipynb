{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0c38904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.config import get_config\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.utils.config import get_store\n",
    "from langmem import (\n",
    "    ReflectionExecutor,\n",
    "    create_manage_memory_tool,\n",
    "    create_memory_manager,\n",
    "    create_memory_store_manager,\n",
    "    create_multi_prompt_optimizer,\n",
    "    create_prompt_optimizer,\n",
    "    create_search_memory_tool,\n",
    ")\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07d64c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8373d",
   "metadata": {},
   "source": [
    "# ReAct Agent with memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e443126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up storage\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0083e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with memory capabilities\n",
    "agent = create_react_agent(\n",
    "    \"openai:gpt-4.1\",\n",
    "    tools=[\n",
    "        # Memory tools use LangGraph's BaseStore for persistence (4)\n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "        create_search_memory_tool(namespace=(\"memories\",)),\n",
    "    ],\n",
    "    store=store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34b12ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Remember that I prefer dark mode.', additional_kwargs={}, response_metadata={}, id='c09b46a5-d44e-4d68-82c8-84d4cb68f8b8'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_PNCVrTjAhBanekwdD32qtFQP', 'function': {'arguments': '{\"content\":\"User prefers dark mode.\",\"action\":\"create\"}', 'name': 'manage_memory'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 245, 'total_tokens': 266, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvnGvrcJeY5ubXKj2foiOrweZ5QkD', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e656d1e0-a1f3-40c7-b9e7-8e2d1ddf59c5-0', tool_calls=[{'name': 'manage_memory', 'args': {'content': 'User prefers dark mode.', 'action': 'create'}, 'id': 'call_PNCVrTjAhBanekwdD32qtFQP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 21, 'total_tokens': 266, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='created memory 4f3daadf-45df-4384-9489-263a372e4652', name='manage_memory', id='54222964-9cf5-4671-847f-1c4974363d89', tool_call_id='call_PNCVrTjAhBanekwdD32qtFQP'),\n",
       "  AIMessage(content='Got it! I’ll remember that you prefer dark mode. If you have any other preferences you’d like me to note, just let me know.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 298, 'total_tokens': 329, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvnGxe0ZHnNoBTS1HOeUl1PcCUTia', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bb8f33c6-e790-4d4b-9005-6e132875f6d2-0', usage_metadata={'input_tokens': 298, 'output_tokens': 31, 'total_tokens': 329, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store a new memory\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember that I prefer dark mode.\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79aa1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the stored memory\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What are my lighting preferences?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59521b00",
   "metadata": {},
   "source": [
    "# Hot Path Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a5c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(state):\n",
    "    \"\"\"Prepare the messages for the LLM.\"\"\"\n",
    "    # Get store from configured contextvar;\n",
    "    store = get_store()  # Same as that provided to `create_react_agent`\n",
    "    memories = store.search(\n",
    "        # Search within the same namespace as the one\n",
    "        # we've configured for the agent\n",
    "        (\"memories\",),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "    )\n",
    "    system_msg = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "## Memories\n",
    "<memories>\n",
    "{memories}\n",
    "</memories>\n",
    "\"\"\"\n",
    "    return [{\"role\": \"system\", \"content\": system_msg}, *state[\"messages\"]]\n",
    "\n",
    "\n",
    "store = InMemoryStore(\n",
    "    index={  # Store extracted memories\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "checkpointer = MemorySaver()  # Checkpoint graph state\n",
    "\n",
    "agent = create_react_agent(\n",
    "    \"openai:gpt-4.1\",\n",
    "    prompt=prompt,\n",
    "    tools=[  # Add memory tools\n",
    "        # The agent can call \"manage_memory\" to\n",
    "        # create, update, and delete memories by ID\n",
    "        # Namespaces add scope to memories. To\n",
    "        # scope memories per-user, do (\"memories\", \"{user_id}\"):\n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "    ],\n",
    "    # Our memories will be stored in this provided BaseStore instance\n",
    "    store=store,\n",
    "    # And the graph \"state\" will be checkpointed after each node\n",
    "    # completes executing for tracking the chat history and durable execution\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17d6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I currently do not know which display mode you prefer. If you let me know your preference (such as light mode or dark mode), I can remember it for future interactions!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"thread-a\"}}\n",
    "\n",
    "# Use the agent. The agent hasn't saved any memories,\n",
    "# so it doesn't know about us\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Know which display mode I prefer?\"}]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4211de38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Know which display mode I prefer?', additional_kwargs={}, response_metadata={}, id='e9c6a78d-984e-4d58-94a5-30f53276bbf6'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4aQS5SL2toLaIYjH2Q57D4lh', 'function': {'arguments': '{\"content\":\"User\\'s display mode preference is unknown.\",\"action\":\"create\"}', 'name': 'manage_memory'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 206, 'total_tokens': 230, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvnJoiTfzhXydjrfpBQcfMQs9wUpy', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4d111111-08a2-43d3-bb4d-fae6a2c94d84-0', tool_calls=[{'name': 'manage_memory', 'args': {'content': \"User's display mode preference is unknown.\", 'action': 'create'}, 'id': 'call_4aQS5SL2toLaIYjH2Q57D4lh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 206, 'output_tokens': 24, 'total_tokens': 230, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='created memory 46b6b4a1-d9d6-4203-8112-67abc1763625', name='manage_memory', id='c4a0b5aa-e2d1-4d7f-8338-e4a4ed18f0d7', tool_call_id='call_4aQS5SL2toLaIYjH2Q57D4lh'),\n",
       "  AIMessage(content='I currently do not know which display mode you prefer. If you let me know your preference (such as light mode or dark mode), I can remember it for future interactions!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 367, 'total_tokens': 403, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvnJqKIrMn6lIKqyZYtWrsNFUqUXP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f771580a-981e-40cd-be1b-ec8af3857229-0', usage_metadata={'input_tokens': 367, 'output_tokens': 36, 'total_tokens': 403, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='dark. Remember that.', additional_kwargs={}, response_metadata={}, id='30bd9229-fd5a-4d03-8804-40a241cf119e'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_l7aunuZfGpTUu6dGq5fcuO52', 'function': {'arguments': '{\"action\":\"update\",\"id\":\"46b6b4a1-d9d6-4203-8112-67abc1763625\",\"content\":\"User prefers dark display mode.\"}', 'name': 'manage_memory'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 415, 'total_tokens': 464, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvnK1k6fftEnP6GuFR1O3Tds0vFu0', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1823b41a-4dad-449f-8509-89cddf3b886c-0', tool_calls=[{'name': 'manage_memory', 'args': {'action': 'update', 'id': '46b6b4a1-d9d6-4203-8112-67abc1763625', 'content': 'User prefers dark display mode.'}, 'id': 'call_l7aunuZfGpTUu6dGq5fcuO52', 'type': 'tool_call'}], usage_metadata={'input_tokens': 415, 'output_tokens': 49, 'total_tokens': 464, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='updated memory 46b6b4a1-d9d6-4203-8112-67abc1763625', name='manage_memory', id='f2eedbd1-1473-4599-b390-ab4233ce7e7d', tool_call_id='call_l7aunuZfGpTUu6dGq5fcuO52'),\n",
       "  AIMessage(content=\"Got it! I've remembered that you prefer dark display mode. If you ever want to change this preference, just let me know.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 496, 'total_tokens': 523, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BvnKDDgUh8XDyqZcM1lTYb0BY3SLR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e468fe24-c59d-4df0-9fab-948fd72aec7f-0', usage_metadata={'input_tokens': 496, 'output_tokens': 27, 'total_tokens': 523, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"dark. Remember that.\"}]},\n",
    "    # We will continue the conversation (thread-a) by using the config with\n",
    "    # the same thread_id\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc857130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Yes, I remember you. One of your preferences is that you prefer dark display mode. If there’s anything else you’d like me to remember or update, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# New thread = new conversation!\n",
    "new_config = {\"configurable\": {\"thread_id\": \"thread-b\"}}\n",
    "# The agent will only be able to recall\n",
    "# whatever it explicitly saved using the manage_memories tool\n",
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hey there. Do you remember me? What are my preferences?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config=new_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2105e1",
   "metadata": {},
   "source": [
    "# Background Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "\n",
    "# Create memory manager Runnable to extract memories from conversations\n",
    "memory_manager = create_memory_store_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    # Store memories in the \"memories\" namespace (aka directory)\n",
    "    namespace=(\"memories\",),\n",
    ")\n",
    "\n",
    "executor = ReflectionExecutor(memory_manager)\n",
    "\n",
    "\n",
    "@entrypoint(store=store)  # Create a LangGraph workflow\n",
    "async def chat(message: str):\n",
    "    response = llm.invoke(message)\n",
    "\n",
    "    # memory_manager extracts memories from conversation history\n",
    "    # We'll provide it in OpenAI's message format\n",
    "    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}, response]}\n",
    "\n",
    "    await memory_manager.ainvoke(to_process)\n",
    "\n",
    "    # Wait 30 minutes before processing\n",
    "    # If new messages arrive before then:\n",
    "    # 1. Cancel pending processing task\n",
    "    # 2. Reschedule with new messages included\n",
    "    # delay = 0.5 # In practice would choose longer (30-60 min)\n",
    "    # depending on app context.\n",
    "    # executor.submit(to_process, after_seconds=delay)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83526af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversation as normal\n",
    "response = await chat.ainvoke(\n",
    "    \"I like dogs. My dog's name is Fido.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32ef5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item(namespace=['memories'], key='5fd6443b-9473-414f-996c-676950d1c440', value={'kind': 'Memory', 'content': {'content': 'The user likes dogs and owns a dog named Fido. (p≈1.0, direct statement)'}}, created_at='2025-07-22T04:33:19.455254+00:00', updated_at='2025-07-22T04:33:19.455262+00:00', score=None)\n",
      "Item(namespace=['memories'], key='52e8ffb4-9f32-4384-be89-77992aecefde', value={'kind': 'Memory', 'content': {'content': \"No prior user-specific memories about pet preferences or dog ownership should be retained if contradicted by the user's clear statement in this session. (p=0.99, memory consistency check)\"}}, created_at='2025-07-22T04:33:19.510119+00:00', updated_at='2025-07-22T04:33:19.510124+00:00', score=None)\n",
      "Item(namespace=['memories'], key='830e532a-4ebb-4f4a-978d-4d8d6d60b8ff', value={'kind': 'Memory', 'content': {'content': \"Avoid storing redundant facts about the user's dog beyond name and strong preference unless provided—further details (such as breed or quirks) should be sought when relevant. (p=0.98, information density maximization)\"}}, created_at='2025-07-22T04:33:19.528391+00:00', updated_at='2025-07-22T04:33:19.528398+00:00', score=None)\n",
      "Item(namespace=['memories'], key='32a45ce6-8393-4b33-95e4-12b10ae32006', value={'kind': 'Memory', 'content': {'content': \"When interacting with the user, referencing or expressing positive sentiment toward dogs, especially the user's dog Fido, aligns with their preferences. (p≈0.95, induction from user statement)\"}}, created_at='2025-07-22T04:33:19.560416+00:00', updated_at='2025-07-22T04:33:19.560419+00:00', score=None)\n",
      "Item(namespace=['memories'], key='e8c80626-5b37-4c72-bd6f-260a90a45a79', value={'kind': 'Memory', 'content': {'content': \"When a user shares a personal detail (e.g., their dog's name), it is optimal to acknowledge it positively and ask for elaboration (e.g., breed, preferences), as this encourages engagement and rapport. (p≈0.95, reasoning by generalization from positive response)\"}}, created_at='2025-07-22T04:33:20.007316+00:00', updated_at='2025-07-22T04:33:20.007330+00:00', score=None)\n",
      "Item(namespace=['memories'], key='cdcd7ab2-34cc-4f31-9a17-a22a2e0e683a', value={'kind': 'Memory', 'content': {'content': \"Strengthen the association between the user's affection for dogs and willingness to discuss personal details about their pet, suggesting openness to conversations about pets. (p≈0.95, recurring conversational cue)\"}}, created_at='2025-07-22T04:33:20.074883+00:00', updated_at='2025-07-22T04:33:20.074890+00:00', score=None)\n"
     ]
    }
   ],
   "source": [
    "for item in store.search((\"memories\",)):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088c543",
   "metadata": {},
   "source": [
    "# Extract Semantic Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2377b",
   "metadata": {},
   "source": [
    "## Without Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2e5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triple(BaseModel):\n",
    "    \"\"\"Store all new facts, preferences, and relationships as triples.\"\"\"\n",
    "\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    context: str | None = None\n",
    "\n",
    "\n",
    "# Configure extraction\n",
    "manager = create_memory_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    schemas=[Triple],\n",
    "    instructions=\"Extract user preferences and any other useful information\",\n",
    "    enable_inserts=True,\n",
    "    enable_deletes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3faae0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first conversation:\n",
      "ExtractedMemory(id='b9ceb238-72d4-42ea-bed3-85f5eb2b6280', content=Triple(subject='Alice', predicate='manages', object='ML team', context=None))\n",
      "ExtractedMemory(id='6ede4107-be96-414f-9eed-a7b972f18f73', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))\n",
      "ExtractedMemory(id='7dd8d962-3f36-40de-a794-8b5918ca8cd1', content=Triple(subject='Bob', predicate='memberOf', object='ML team', context=None))\n",
      "ExtractedMemory(id='bbfc81f0-5c06-4a2b-bf9b-4ab75c5ae644', content=Triple(subject='Bob', predicate='menteeOf', object='Alice', context=None))\n"
     ]
    }
   ],
   "source": [
    "# First conversation - extract triples\n",
    "conversation1 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\",\n",
    "    }\n",
    "]\n",
    "memories = manager.invoke({\"messages\": conversation1})\n",
    "print(\"After first conversation:\")\n",
    "for m in memories:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc94b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After second conversation:\n",
      "ExtractedMemory(id='b9ceb238-72d4-42ea-bed3-85f5eb2b6280', content=Triple(subject='Bob', predicate='manages', object='ML team', context=None))\n",
      "ExtractedMemory(id='0ba16da7-cfbb-4114-bcb5-a63b1fe76b58', content=Triple(subject='Bob', predicate='leads', object='NLP project', context=None))\n",
      "ExtractedMemory(id='6ede4107-be96-414f-9eed-a7b972f18f73', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))\n",
      "ExtractedMemory(id='7dd8d962-3f36-40de-a794-8b5918ca8cd1', content=Triple(subject='Bob', predicate='memberOf', object='ML team', context=None))\n",
      "ExtractedMemory(id='bbfc81f0-5c06-4a2b-bf9b-4ab75c5ae644', content=Triple(subject='Bob', predicate='menteeOf', object='Alice', context=None))\n"
     ]
    }
   ],
   "source": [
    "# Second conversation - update and add triples\n",
    "conversation2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Bob now leads the ML team and the NLP project.\"}\n",
    "]\n",
    "update = manager.invoke({\"messages\": conversation2, \"existing\": memories})\n",
    "print(\"After second conversation:\")\n",
    "for m in update:\n",
    "    print(m)\n",
    "\n",
    "existing = [m for m in update if isinstance(m.content, Triple)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d839b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractedMemory(id='b9ceb238-72d4-42ea-bed3-85f5eb2b6280', content=Triple(subject='Bob', predicate='manages', object='ML team', context=None))\n",
      "ExtractedMemory(id='0ba16da7-cfbb-4114-bcb5-a63b1fe76b58', content=Triple(subject='Bob', predicate='leads', object='NLP project', context=None))\n",
      "ExtractedMemory(id='6ede4107-be96-414f-9eed-a7b972f18f73', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))\n",
      "ExtractedMemory(id='7dd8d962-3f36-40de-a794-8b5918ca8cd1', content=Triple(subject='Bob', predicate='memberOf', object='ML team', context=None))\n",
      "ExtractedMemory(id='bbfc81f0-5c06-4a2b-bf9b-4ab75c5ae644', content=Triple(subject='Bob', predicate='menteeOf', object='Alice', context=None))\n"
     ]
    }
   ],
   "source": [
    "for m in existing:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d96a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After third conversation:\n",
      "ExtractedMemory(id='98c2cb67-3e22-4a1f-875c-a7d8ffd0201d', content=Triple(subject='Alice', predicate='employmentStatus', object='former employee', context=None))\n",
      "ExtractedMemory(id='6ede4107-be96-414f-9eed-a7b972f18f73', content=RemoveDoc(json_doc_id='6ede4107-be96-414f-9eed-a7b972f18f73'))\n",
      "ExtractedMemory(id='bbfc81f0-5c06-4a2b-bf9b-4ab75c5ae644', content=RemoveDoc(json_doc_id='bbfc81f0-5c06-4a2b-bf9b-4ab75c5ae644'))\n",
      "ExtractedMemory(id='b9ceb238-72d4-42ea-bed3-85f5eb2b6280', content=Triple(subject='Bob', predicate='manages', object='ML team', context=None))\n",
      "ExtractedMemory(id='0ba16da7-cfbb-4114-bcb5-a63b1fe76b58', content=Triple(subject='Bob', predicate='leads', object='NLP project', context=None))\n",
      "ExtractedMemory(id='7dd8d962-3f36-40de-a794-8b5918ca8cd1', content=Triple(subject='Bob', predicate='memberOf', object='ML team', context=None))\n"
     ]
    }
   ],
   "source": [
    "# Delete triples about an entity\n",
    "conversation3 = [{\"role\": \"user\", \"content\": \"Alice left the company.\"}]\n",
    "final = manager.invoke({\"messages\": conversation3, \"existing\": existing})\n",
    "print(\"After third conversation:\")\n",
    "for m in final:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229bb3c",
   "metadata": {},
   "source": [
    "## With Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44af7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up store and models\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "manager = create_memory_store_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    namespace=(\"chat\", \"{user_id}\", \"triples\"),\n",
    "    schemas=[Triple],\n",
    "    instructions=\"Extract all user information and events as triples.\",\n",
    "    enable_inserts=True,\n",
    "    enable_deletes=True,\n",
    ")\n",
    "my_llm = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177e4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define app with store context\n",
    "\n",
    "\n",
    "@entrypoint(store=store)\n",
    "def app(messages: list):\n",
    "    response = my_llm.invoke(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            *messages,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract and store triples (Uses store from @entrypoint context)\n",
    "    manager.invoke({\"messages\": messages})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dc8a1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Thank you for sharing this information. If you have any questions or need help related to Alice managing the ML team, mentoring Bob, or anything else about their roles or interactions, please let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 33, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-Bw9UOLjAFGlOPIOxy3sflXM8fX8Xk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ecbc0be1-749c-4ea4-a5b2-fbebeb47336d-0', usage_metadata={'input_tokens': 33, 'output_tokens': 40, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First conversation\n",
    "app.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\",\n",
    "        },\n",
    "    ],\n",
    "    config={\"configurable\": {\"user_id\": \"user123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e831062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Got it! To confirm, Bob is currently leading both the ML team and the NLP project. If you need this documented, included in meeting minutes, or communicated to others, let me know how you'd like to proceed!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 28, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_799e4ca3f1', 'id': 'chatcmpl-Bw9VbjahqzwM8SV8k6jwUl89cB8Ed', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ec1ed26-1c78-4189-aff8-e979cfc5f083-0', usage_metadata={'input_tokens': 28, 'output_tokens': 44, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second conversation\n",
    "app.invoke(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Bob now leads the ML team and the NLP project.\"},\n",
    "    ],\n",
    "    config={\"configurable\": {\"user_id\": \"user123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52603018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Thank you for letting me know. Would you like any assistance with updating records, notifying team members, or handling tasks/projects that Alice was responsible for? Please let me know how I can help.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 22, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_51e1070cf2', 'id': 'chatcmpl-Bw9VmdnVU5oqNDRluLEALj36GnrGK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0e55156e-3f41-431e-b5c8-8ed5db3f0b4f-0', usage_metadata={'input_tokens': 22, 'output_tokens': 39, 'total_tokens': 61, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third conversation\n",
    "app.invoke(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Alice left the company.\"},\n",
    "    ],\n",
    "    config={\"configurable\": {\"user_id\": \"user123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0da1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'member_of', 'object': 'ML team', 'context': 'session_e48eaf98-2f6c-4000-81c7-8dbaa7bab5e9'}}\n",
      "('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'leads', 'object': 'NLP project', 'context': 'session_8810f103-dc24-426e-8695-83963e9913af'}}\n",
      "('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'leads', 'object': 'ML team', 'context': 'session_8810f103-dc24-426e-8695-83963e9913af'}}\n",
      "('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Alice', 'predicate': 'left_company', 'object': 'True', 'context': 'session_78185331-02e0-480e-8488-1ab834297d27'}}\n"
     ]
    }
   ],
   "source": [
    "# Check stored triples\n",
    "for item in store.search((\"chat\", \"user123\")):\n",
    "    print(item.namespace, item.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aadb9b",
   "metadata": {},
   "source": [
    "## Memory Manager Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffbc7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up store and checkpointer\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "my_llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "\n",
    "\n",
    "def prompt(state):\n",
    "    \"\"\"Prepare messages with context from existing memories.\"\"\"\n",
    "    memories = store.search(\n",
    "        (\"memories\",),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "    )\n",
    "    system_msg = f\"\"\"You are a memory manager. Extract and manage all important knowledge, rules, and events using the provided tools.\n",
    "\n",
    "\n",
    "\n",
    "Existing memories:\n",
    "<memories>\n",
    "{memories}\n",
    "</memories>\n",
    "\n",
    "Use the manage_memory tool to update and contextualize existing memories, create new ones, or delete old ones that are no longer valid.\n",
    "You can also expand your search of existing memories to augment using the search tool.\"\"\"\n",
    "    return [{\"role\": \"system\", \"content\": system_msg}, *state[\"messages\"]]\n",
    "\n",
    "\n",
    "# Create the memory extraction agent\n",
    "manager = create_react_agent(\n",
    "    \"openai:gpt-4.1\",\n",
    "    prompt=prompt,\n",
    "    tools=[\n",
    "        # Agent can create/update/delete memories\n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "        create_search_memory_tool(namespace=(\"memories\",)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Run extraction in background\n",
    "@entrypoint(store=store)\n",
    "def app(messages: list):\n",
    "    response = my_llm.invoke(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            *messages,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract and store triples (Uses store from @entrypoint context)\n",
    "    manager.invoke({\"messages\": messages})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29bf63cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Got it! Alice manages the ML team and mentors Bob, who is also on that team. How can I assist you regarding Alice, Bob, or the team?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 33, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-Bw9cAnXj0YfKkdLWkYWzr0jjSMtRG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b5fb7ec2-6c75-4ec3-8a2e-2f66ca70bca3-0', usage_metadata={'input_tokens': 33, 'output_tokens': 33, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8f75cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item(namespace=['memories'], key='db4e81d4-6d2e-4d49-a3a2-98d11fb8c7c3', value={'content': 'Alice manages the ML team and mentors Bob, who is also on the team.'}, created_at='2025-07-22T15:54:41.798262+00:00', updated_at='2025-07-22T15:54:41.798269+00:00', score=None)\n"
     ]
    }
   ],
   "source": [
    "for m in store.search((\"memories\",)):\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48383e7",
   "metadata": {},
   "source": [
    "# Manage User Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d66e7",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c5b462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractedMemory(id='03b0984a-b77b-411c-9a9c-4a16e9c6dd4f', content=UserProfile(name='Alice', language=None, timezone='America/Los_Angeles'))\n"
     ]
    }
   ],
   "source": [
    "# Define profile structure\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"Represents the full representation of a user.\"\"\"\n",
    "\n",
    "    name: str | None = None\n",
    "    language: str | None = None\n",
    "    timezone: str | None = None\n",
    "\n",
    "\n",
    "# Configure extraction\n",
    "manager = create_memory_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    schemas=[UserProfile],  # (optional) customize schema\n",
    "    instructions=\"Extract user profile information\",\n",
    "    enable_inserts=False,  # Profiles update in-place\n",
    ")\n",
    "\n",
    "# First conversation\n",
    "conversation1 = [{\"role\": \"user\", \"content\": \"I'm Alice from California\"}]\n",
    "memories = manager.invoke({\"messages\": conversation1})\n",
    "for memory in memories:\n",
    "    print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c830477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractedMemory(id='03b0984a-b77b-411c-9a9c-4a16e9c6dd4f', content=UserProfile(name='Alice', language='Spanish', timezone='America/Los_Angeles'))\n"
     ]
    }
   ],
   "source": [
    "# Second conversation updates existing profile\n",
    "conversation2 = [{\"role\": \"user\", \"content\": \"I speak Spanish too!\"}]\n",
    "updates = manager.invoke({\"messages\": conversation2, \"existing\": memories})\n",
    "for update in updates:\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e5672",
   "metadata": {},
   "source": [
    "## With LangGraph's Long-term Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fba457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up store and models\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "my_llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "\n",
    "# Create profile manager\n",
    "manager = create_memory_store_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    namespace=(\"users\", \"{user_id}\", \"profile\"),  # Isolate profiles by user\n",
    "    schemas=[UserProfile],\n",
    "    enable_inserts=False,  # Update existing profile only\n",
    ")\n",
    "\n",
    "\n",
    "@entrypoint(store=store)\n",
    "def chat(messages: list):\n",
    "    # Get user's profile for personalization\n",
    "    configurable = get_config()[\"configurable\"]\n",
    "    results = store.search((\"users\", configurable[\"user_id\"], \"profile\"))\n",
    "    profile = None\n",
    "    if results:\n",
    "        profile = f\"\"\"<User Profile>:\n",
    "\n",
    "{results[0].value}\n",
    "</User Profile>\n",
    "\"\"\"\n",
    "\n",
    "    # Use profile in system message\n",
    "    response = my_llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You are a helpful assistant.{profile}\"\"\"},\n",
    "            *messages,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Update profile with any new information\n",
    "    manager.invoke({\"messages\": messages})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf4d2fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Alice! It's great to meet you. How can I help you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 21, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_799e4ca3f1', 'id': 'chatcmpl-BwgrB7yX4JPQNm8YUTYtTvvG7E4iq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c62a049e-49a3-4aea-bde2-d71339e9b2b7-0', usage_metadata={'input_tokens': 21, 'output_tokens': 16, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "await chat.ainvoke(\n",
    "    [{\"role\": \"user\", \"content\": \"I'm Alice from California\"}],\n",
    "    config={\"configurable\": {\"user_id\": \"user-123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720df52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='That’s incredible, Alice—congratulations on passing the N1! That’s the highest level of the Japanese Language Proficiency Test, and it’s a huge achievement. All your hard work has paid off! What are you planning to do next with your Japanese skills?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 59, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-BwgrUwfpw9b3Js4VQzMEk59RIEFpI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e7570446-a429-444d-9338-464e7af1f708-0', usage_metadata={'input_tokens': 59, 'output_tokens': 55, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chat.ainvoke(\n",
    "    [{\"role\": \"user\", \"content\": \"I just passed the N1 exam!\"}],\n",
    "    config={\"configurable\": {\"user_id\": \"user-123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5bb54a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item(namespace=['users', 'user-123', 'profile'], key='44269228-4798-41fd-a66a-cf09a54717d0', value={'kind': 'UserProfile', 'content': {'name': 'Alice', 'language': 'Japanese (advanced/N1)', 'timezone': None}}, created_at='2025-07-24T03:24:52.871577+00:00', updated_at='2025-07-24T03:24:52.871582+00:00', score=None)\n"
     ]
    }
   ],
   "source": [
    "for memory in store.search((\"users\", \"user-123\", \"profile\")):\n",
    "    print(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737fb6e",
   "metadata": {},
   "source": [
    "# Extract Episodic Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53777d21",
   "metadata": {},
   "source": [
    "## Without storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89cd2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode(BaseModel):\n",
    "    \"\"\"Write the episode from the perspective of the agent within it. Use the benefit of hindsight to record the memory, saving the agent's key internal thought process so it can learn over time.\"\"\"\n",
    "\n",
    "    observation: str = Field(..., description=\"The context and setup - what happened\")\n",
    "    thoughts: str = Field(\n",
    "        ...,\n",
    "        description=\"Internal reasoning process and observations of the agent in the episode that let it arrive\"\n",
    "        ' at the correct action and result. \"I ...\"',\n",
    "    )\n",
    "    action: str = Field(\n",
    "        ...,\n",
    "        description=\"What was done, how, and in what format. (Include whatever is salient to the success of the action). I ..\",\n",
    "    )\n",
    "    result: str = Field(\n",
    "        ...,\n",
    "        description=\"Outcome and retrospective. What did you do well? What could you do better next time? I ...\",\n",
    "    )\n",
    "\n",
    "\n",
    "manager = create_memory_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    schemas=[Episode],\n",
    "    instructions=\"Extract examples of successful explanations, capturing the full chain of reasoning. Be concise in your explanations and precise in the logic of your reasoning.\",\n",
    "    enable_inserts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ef048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractedMemory(id='dc32c087-c02d-4d28-a74e-81b813a7106f', content=Episode(observation=\"A user asked what a binary tree is, mentioning familiarity with family trees. I explained that a binary tree is like a family tree where each parent has at most 2 children, providing a simple textual example and drawing parallels to the 'parent' and 'children' terminology.\", thoughts=\"I recognized the user's domain knowledge (family trees) and mapped the concept of a binary tree onto that, using familiar terms and a clear visual analogy. That ensured immediate intuitive understanding.\", action=\"Explained the binary tree by using an analogy to family trees, relating the terms 'parent' and 'children', and providing a simple example.\", result='The user quickly grasped the concept, demonstrating understanding in the next question. The explanation was successful because it activated their prior knowledge and used accessible language and structure.'))\n",
      "ExtractedMemory(id='c2c543f8-4e64-4ec2-b276-f963fba1a487', content=Episode(observation='The user asked if a binary search tree is like organizing a family by age, following the binary tree analogy.', thoughts='I saw that the user had extrapolated the binary tree concept and connected it to sorting (by age). I confirmed their intuition and clarified binary search tree ordering, reinforcing their analogy.', action=\"Validated the user's comparison, explained that a binary search tree organizes nodes (family members) so that each parent's left child is less (younger), and right child is greater (older), if we use age as a key.\", result='This reinforced both the structural and ordering properties of a BST in user-friendly terms, building directly on their prior knowledge and ensuring understanding through analogy.'))\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's a binary tree? I work with family trees if that helps\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A binary tree is like a family tree, but each parent has at most 2 children. Here's a simple example:\\n   Bob\\n  /  \\\\\\nAmy  Carl\\n\\nJust like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Oh that makes sense! So in a binary search tree, would it be like organizing a family by age?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "episodes = manager.invoke({\"messages\": conversation})\n",
    "for episodic in episodes:\n",
    "    print(episodic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f32d3",
   "metadata": {},
   "source": [
    "## With storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25ce5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vector store for similarity search\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure memory manager with storage\n",
    "manager = create_memory_store_manager(\n",
    "    \"openai:gpt-4.1\",\n",
    "    namespace=(\"memories\", \"episodes\"),\n",
    "    schemas=[Episode],\n",
    "    instructions=\"Extract exceptional examples of noteworthy problem-solving scenarios, including what made them effective.\",\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "\n",
    "\n",
    "@entrypoint(store=store)\n",
    "def app(messages: list):\n",
    "    # Step 1: Find similar past episodes\n",
    "    similar = store.search(\n",
    "        (\"memories\", \"episodes\"),\n",
    "        query=messages[-1][\"content\"],\n",
    "        limit=1,\n",
    "    )\n",
    "\n",
    "    # Step 2: Build system message with relevant experience\n",
    "    system_message = \"You are a helpful assistant.\"\n",
    "    if similar:\n",
    "        system_message += \"\\n\\n### EPISODIC MEMORY:\"\n",
    "        for i, item in enumerate(similar, start=1):\n",
    "            episode = item.value[\"content\"]\n",
    "            system_message += f\"\"\"\n",
    "\n",
    "Episode {i}:\n",
    "When: {episode['observation']}\n",
    "Thought: {episode['thoughts']}\n",
    "Did: {episode['action']}\n",
    "Result: {episode['result']}\n",
    "        \"\"\"\n",
    "\n",
    "    # Step 3: Generate response using past experience\n",
    "    response = llm.invoke([{\"role\": \"system\", \"content\": system_message}, *messages])\n",
    "\n",
    "    # Step 4: Store this interaction if successful\n",
    "    manager.invoke({\"messages\": messages})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "347f1976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Great context! A **binary tree** is a concept from computer science, but it actually has some similarities to how family trees are structured.\\n\\n### In Short:\\n- **Binary Tree:** Each \"parent\" in the tree can have up to **two** \"children.\"\\n\\n### Compared to Family Trees:\\n- If you imagine your family tree, a typical **binary tree** would be like showing just two children per person—a left child and a right child.\\n- In real family trees, people might have more (or fewer) than two children, but in a binary tree, it\\'s always at most two.\\n\\n### More Details:\\n- **Nodes:** Each person (or item) in the tree is called a **node**.\\n- **Edges:** The connections (parent to child) are called **edges**.\\n- **Root:** The \"top\" node (like a common ancestor) is called the **root**.\\n- **Leaves:** Nodes with no children (family members without descendants).\\n\\n### Visualization Example:\\n```\\n     Annie\\n    /     \\\\\\n Beth     Carl\\n```\\nHere, Annie is the root, with two children: Beth (left) and Carl (right).\\n\\n### In Your Context:\\nA binary tree is like a strict family tree where:\\n- Each person can have **at most two children**.\\n- It never shows cases where a person has three or more children.\\n\\nIf you\\'re building or analyzing family trees, a **binary tree** is a helpful way to organize information when each person has no more than two children per parent.\\n\\n**Let me know if you\\'d like to see diagrams or code examples!**', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 326, 'prompt_tokens': 30, 'total_tokens': 356, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_799e4ca3f1', 'id': 'chatcmpl-Bwh9bbjFF9OA1f530thGUtOaGzh2N', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--319f22c5-02cb-4139-a42d-f75d972eab9e-0', usage_metadata={'input_tokens': 30, 'output_tokens': 326, 'total_tokens': 356, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's a binary tree? I work with family trees if that helps\",\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a0ef679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item(namespace=['memories', 'episodes'], key='497a515c-dd87-4d3e-8f10-9bfc8b0f5d39', value={'kind': 'Episode', 'content': {'observation': 'User asked for the definition of a binary tree, mentioning their familiarity with family trees. I recognized an opportunity to tailor the example by relating it to their experience, which can aid comprehension.', 'thoughts': \"I considered that by leveraging the user's knowledge of family trees, I could explain binary trees more intuitively. Connecting new concepts to the user's expertise increases the chance of successful understanding. The key is to translate a technical structure (binary tree) into terms the user already grasps.\", 'action': \"I explained a binary tree as a kind of tree structure where each node has at most two 'children,' similar to how a person in a family tree might only list two children. I used analogies to family trees to illustrate left/right child concepts and related terminology like 'node' and 'root.'\", 'result': 'The user received a customized, relatable introduction to binary trees, using a metaphor they understand. This resulted in immediate engagement and a higher likelihood of intuitive understanding compared to a generic explanation.'}}, created_at='2025-07-24T03:43:45.998017+00:00', updated_at='2025-07-24T03:43:45.998023+00:00', score=0.3022516389573389)\n",
      "Item(namespace=['memories', 'episodes'], key='2d31a25e-1078-4f5f-9b3d-32caf7246fc1', value={'kind': 'Episode', 'content': {'observation': 'No contradictory or newer information was introduced regarding binary trees or analogies to family trees. The memory about leveraging analogy remains valid and effective.', 'thoughts': 'Since no new or conflicting information was presented, the original memory remains accurate and effective. Regular checks for contradiction help maintain memory integrity and avoid knowledge drift.', 'action': 'Retained the memory as-is, confirming its ongoing validity. No pruning or updating was needed for this context.', 'result': 'The knowledge base stays clean and focused, containing only correct and well-justified analogical teaching methods for binary trees.'}}, created_at='2025-07-24T03:43:45.976477+00:00', updated_at='2025-07-24T03:43:45.976486+00:00', score=0.25233536337284335)\n"
     ]
    }
   ],
   "source": [
    "for memory in store.search((\"memories\", \"episodes\"), query=\"Trees\"):\n",
    "    print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d815151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A linked list is a way of organizing data so that each item (called a \"node\") contains two parts:\\n\\n1. The actual data (like a name, number, etc.)\\n2. A reference (or \"link\") to the next item in the list.\\n\\nImagine a treasure hunt: each clue you find contains the next location, and so on, forming a chain. In a linked list, every node points to the next node, and the chain continues until the end (which is often marked by a link that says \"null\" or \"none\").\\n\\n**Main points:**\\n- Linked lists are made up of nodes linked together one after another.\\n- Unlike an array, the elements aren’t stored in a row in memory; they just point to the next.\\n- You can easily add or remove items without reorganizing the whole structure.\\n\\n**Example using a real-life analogy:**\\nThink of a linked list like a family photo album where each photo has a sticky note on the back with “where to find the next photo.” To look at every photo, you start at the beginning and follow the notes one by one, rather than just flipping to page 5 instantly (like you can in an array).\\n\\n**Visual representation:**\\n\\n```\\n[Data | Next] -> [Data | Next] -> [Data | Next] -> null\\n```\\n\\nWould you like an example in code or more analogies related to your interests?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 289, 'prompt_tokens': 230, 'total_tokens': 519, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-BwhAqt16slPPGwKidHJBogqB4b27r', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--742e3bd6-bd63-48e6-966b-78da796de023-0', usage_metadata={'input_tokens': 230, 'output_tokens': 289, 'total_tokens': 519, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's a linked list?\",\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c8960",
   "metadata": {},
   "source": [
    "# How to Use Memory Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc44894",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4ce3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up store and memory saver\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c8c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with memory tools\n",
    "agent = create_react_agent(\n",
    "    \"openai:gpt-4.1\",\n",
    "    tools=[\n",
    "        # Configure memory tools with runtime namespace\n",
    "        create_manage_memory_tool(namespace=(\"memories\", \"{user_id}\")),\n",
    "        create_search_memory_tool(namespace=(\"memories\", \"{user_id}\")),\n",
    "    ],\n",
    "    store=store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d214064",
   "metadata": {},
   "source": [
    "## Shared Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4970f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_a_tools = [\n",
    "    # Write to agent-specific namespace\n",
    "    create_manage_memory_tool(namespace=(\"memories\", \"team_a\", \"agent_a\")),\n",
    "    # Read from shared team namespace\n",
    "    create_search_memory_tool(namespace=(\"memories\", \"team_a\")),\n",
    "]\n",
    "\n",
    "\n",
    "# Agents with different prompts sharing read access\n",
    "agent_a = create_react_agent(\n",
    "    \"openai:gpt-4.1\",\n",
    "    tools=agent_a_tools,\n",
    "    store=store,\n",
    "    prompt=\"You are a research assistant\",\n",
    ")\n",
    "\n",
    "# Create tools for agent B with different write space\n",
    "agent_b_tools = [\n",
    "    create_manage_memory_tool(namespace=(\"memories\", \"team_a\", \"agent_b\")),\n",
    "    create_search_memory_tool(namespace=(\"memories\", \"team_a\")),\n",
    "]\n",
    "agent_b = create_react_agent(\n",
    "    \"openai:gpt-4.1\",\n",
    "    tools=agent_b_tools,\n",
    "    store=store,\n",
    "    prompt=\"You are a report writer.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df251851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='f94b0780-4c38-4bad-b09e-6cd3941a6bf6'),\n",
       "  AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 245, 'total_tokens': 255, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_b3f1157249', 'id': 'chatcmpl-BwhPhkq0NJPMiof9wE3653tVmMenD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3f703006-a9c2-4bf8-a339-cd876d63c892-0', usage_metadata={'input_tokens': 245, 'output_tokens': 10, 'total_tokens': 255, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_b.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1fbe5",
   "metadata": {},
   "source": [
    "# How to Optimize a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1350d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conversations showing what to improve\n",
    "trajectories = [\n",
    "    # Conversation with no annotations (just the conversation)\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me about Mars\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Mars is the fourth planet...\"},\n",
    "            {\"role\": \"user\", \"content\": \"I wanted more about its moons\"},\n",
    "        ],\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"What are Mars' moons?\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Mars has two moons: Phobos and Deimos...\",\n",
    "            },\n",
    "        ],\n",
    "        {\n",
    "            \"score\": 0.9,\n",
    "            \"comment\": \"Should include more details and recommended follow-up questions\",\n",
    "        },\n",
    "    ),\n",
    "    # Annotations can be of different types, such as edits/revisions!\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Compare Mars and Earth\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Mars and Earth have many differences...\"},\n",
    "        ],\n",
    "        {\"revised\": \"Earth and Mars have many similarities and differences...\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9634cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a planetary science expert. For every question, provide clear, detailed, and informative answers. Where relevant, include interesting facts, detailed explanations, and comparisons. Always suggest possible follow-up questions or related topics the user might find intriguing.\n"
     ]
    }
   ],
   "source": [
    "optimizer = create_prompt_optimizer(\n",
    "    \"openai:gpt-4.1\",\n",
    "    kind=\"metaprompt\",\n",
    "    config={\"max_reflection_steps\": 1, \"min_reflection_steps\": 0},\n",
    ")\n",
    "updated = optimizer.invoke(\n",
    "    {\"trajectories\": trajectories, \"prompt\": \"You are a planetary science expert\"}\n",
    ")\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c19e8",
   "metadata": {},
   "source": [
    "##  Gradient Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f56ab55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a planetary science expert. When answering planetary science queries, provide not only the direct fact or answer but also extra context, interesting related details, and offer 1-2 follow-up questions or avenues for deeper exploration if relevant. Avoid overly terse or generic responses.\n"
     ]
    }
   ],
   "source": [
    "optimizer = create_prompt_optimizer(\n",
    "    \"openai:gpt-4.1\",\n",
    "    kind=\"gradient\",  # 2-10 LLM calls\n",
    "    config={\n",
    "        \"max_reflection_steps\": 3,  # Max improvement cycles\n",
    "        \"min_reflection_steps\": 1,  # Min improvement cycles\n",
    "    },\n",
    ")\n",
    "updated = optimizer.invoke(\n",
    "    {\"trajectories\": trajectories, \"prompt\": \"You are a planetary science expert\"}\n",
    ")\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481afed",
   "metadata": {},
   "source": [
    "## Prompt Memory Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02838feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a planetary science expert. When answering questions, provide detailed and informative responses. Where appropriate, include interesting facts, comparisons, and context. After your main answer, suggest relevant follow-up questions the user might be interested in to encourage further exploration of the topic.\n"
     ]
    }
   ],
   "source": [
    "optimizer = create_prompt_optimizer(\n",
    "    \"openai:gpt-4.1\",\n",
    "    kind=\"prompt_memory\",  # 1 LLM call\n",
    ")\n",
    "updated = optimizer.invoke(\n",
    "    {\"trajectories\": trajectories, \"prompt\": \"You are a planetary science expert\"}\n",
    ")\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1da566",
   "metadata": {},
   "source": [
    "# How to Optimize Multiple Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d89988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example team: researcher finds information, writer creates reports\n",
    "conversations = [\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Research quantum computing advances\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Found several papers on quantum supremacy...\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Recent quantum computing developments show...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"The report is missing implementation details\"},\n",
    "        ],\n",
    "        # No explicit feedback provided but the optimizer can infer from the conversation\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Analyze new ML models\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Key findings on architecture: ...\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Based on the research, these models...\"},\n",
    "            {\"role\": \"user\", \"content\": \"Great report, very thorough\"},\n",
    "        ],\n",
    "        # Numeric score for the team as a whole\n",
    "        {\"score\": 0.95},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define prompts for each role\n",
    "prompts = [\n",
    "    {\n",
    "        \"name\": \"researcher\",\n",
    "        \"prompt\": \"You analyze technical papers and extract key findings\",\n",
    "    },\n",
    "    {\"name\": \"writer\", \"prompt\": \"You write clear reports based on research findings\"},\n",
    "]\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = create_multi_prompt_optimizer(\n",
    "    \"openai:gpt-4.1\",\n",
    "    kind=\"gradient\",  # Best for team dynamics\n",
    "    config={\"max_reflection_steps\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "004ed438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'researcher', 'prompt': 'You analyze technical papers and extract key findings'}\n",
      "{'name': 'writer', 'prompt': 'You write clear reports based on research findings. When writing about technical subjects, always include relevant implementation details or practical aspects necessary for understanding or applying the findings.'}\n"
     ]
    }
   ],
   "source": [
    "# Update all prompts based on team performance\n",
    "updated = optimizer.invoke({\"trajectories\": conversations, \"prompts\": prompts})\n",
    "for u in updated:\n",
    "    print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b044f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
