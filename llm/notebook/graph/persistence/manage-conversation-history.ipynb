{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to manage conversation history\n",
    "\n",
    "One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. In order to prevent this from happening, you need to probably manage the conversation history.\n",
    "\n",
    "Note: this guide focuses on how to do this in LangGraph, where you can fully customize how this is done. If you want a more off-the-shelf solution, you can look into functionality provided in LangChain:\n",
    "\n",
    "- [How to filter messages](https://python.langchain.com/v0.2/docs/how_to/filter_messages/)\n",
    "- [How to trim messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the packages we're going to want to use"
   ]
  },
  {
   "cell_type": "code",
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:31:59.564466Z",
     "start_time": "2024-07-19T12:31:59.562061Z"
    }
   },
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install --quiet -U langgraph langchain_anthropic"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for Anthropic (the LLM we will use)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:32:05.986937Z",
     "start_time": "2024-07-19T12:32:05.979868Z"
    }
   },
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Image, display"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "id": "95e25aec-7c9f-4a63-b143-225d0e9a79c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:32:14.120084Z",
     "start_time": "2024-07-19T12:32:14.115228Z"
    }
   },
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "4767ef1c-a7cf-41f8-a301-558988cb7ac5",
   "metadata": {},
   "source": [
    "## Build the agent\n",
    "Let's now build a simple ReAct style agent."
   ]
  },
  {
   "cell_type": "code",
   "id": "378899a9-3b9a-4748-95b6-eb00e0828677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:38:55.663550Z",
     "start_time": "2024-07-19T12:38:55.024434Z"
    }
   },
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    # Don't let the LLM know this though ðŸ˜Š\n",
    "    return [\n",
    "        \"It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "bound_model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"action\", \"__end__\"]:\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"__end__\"\n",
    "    # Otherwise if there is, we continue\n",
    "    return \"action\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:39:12.264034Z",
     "start_time": "2024-07-19T12:39:11.311811Z"
    }
   },
   "cell_type": "code",
   "source": "display(Image(app.get_graph().draw_mermaid_png()))",
   "id": "45b3460064cb2754",
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:39:43.662459Z",
     "start_time": "2024-07-19T12:39:42.028031Z"
    }
   },
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"hi! I'm Tam\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "hi! I'm Tam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hi Tam! How can I assist you today?\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "what's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Your name is Tam! How can I help you today?\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "5d5da4c9-ba8b-46cb-a860-63fe585d15c5",
   "metadata": {},
   "source": [
    "## Filtering messages\n",
    "\n",
    "The most straight-forward thing to do to prevent conversation history from blowing up is to filter the list of messages before they get passed to the LLM. This involves two parts: defining a function to filter messages, and then adding it to the graph. See the example below which defines a really simple `filter_messages` function and then uses it."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb20430f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:46:41.129277Z",
     "start_time": "2024-07-19T12:46:41.098470Z"
    }
   },
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    # Don't let the LLM know this though ðŸ˜Š\n",
    "    return [\n",
    "        \"It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "bound_model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"action\", \"__end__\"]:\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"__end__\"\n",
    "    # Otherwise if there is, we continue\n",
    "    return \"action\"\n",
    "\n",
    "\n",
    "def filter_messages(messages: list):\n",
    "    # This is very simple helper function which only ever uses the last two messages\n",
    "    return messages[-1:]\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = filter_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "52468ebb-4b23-45ac-a98e-b4439f37740a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:46:53.482640Z",
     "start_time": "2024-07-19T12:46:51.418828Z"
    }
   },
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"hi! I'm Tam\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# (because we set `messages[-1:]` in the filter messages argument)\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "hi! I'm Tam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hi Tam! How can I assist you today?\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "what's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "I'm sorry, but I don't know your name. How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "454102b6-7112-4710-aa08-ba675e8be14c",
   "metadata": {},
   "source": [
    "In the above example we defined the `filter_messages` function ourselves. We also provide off-the-shelf ways to trim and filter messages in LangChain. \n",
    "\n",
    "- [How to filter messages](https://python.langchain.com/v0.2/docs/how_to/filter_messages/)\n",
    "- [How to trim messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686861bb-ec32-46f3-b7b3-fdac106f22f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
