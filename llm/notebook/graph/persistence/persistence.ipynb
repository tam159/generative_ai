{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to add persistence (\"memory\") to your graph\n",
    "\n",
    "Many AI applications need memory to share context across multiple interactions. In LangGraph, memory is provided for any [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) through [Checkpointers](https://langchain-ai.github.io/langgraph/reference/checkpoints/).\n",
    "\n",
    "When creating any LangGraph workflow, you can set them up to persist their state by doing using the following:\n",
    "\n",
    "1. A [Checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#basecheckpointsaver), such as the [AsyncSqliteSaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaver)\n",
    "2. Call `compile(checkpointer=my_checkpointer)` when compiling the graph.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "builder = StateGraph(....)\n",
    "# ... define the graph\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "...\n",
    "```\n",
    "\n",
    "This works for [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph) and all its subclasses, such as [MessageGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#messagegraph).\n",
    "\n",
    "Below is an example.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the <code>create_react_agent(model, tools=tool, checkpointer=checkpointer)</code> (<a href=\"https://langchain-ai.github.io/langgraph/reference/prebuilt/#create_react_agent\">API doc</a>) constructor. This may be more appropriate if you are used to LangChainâ€™s <a href=\"https://python.langchain.com/v0.2/docs/how_to/agent_executor/#concepts\">AgentExecutor</a> class.\n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:13:55.095514Z",
     "start_time": "2024-07-19T12:13:55.093246Z"
    }
   },
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install --quiet -U langgraph langchain_anthropic"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the search tool we will use)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:37.553341Z",
     "start_time": "2024-07-19T12:26:37.550688Z"
    }
   },
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Image, display"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "id": "95e25aec-7c9f-4a63-b143-225d0e9a79c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:38.410634Z",
     "start_time": "2024-07-19T12:26:38.407103Z"
    }
   },
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "4cf509bc",
   "metadata": {},
   "source": [
    "## Set up the State\n",
    "\n",
    "The state is the interface for all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "id": "14619607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:41.020934Z",
     "start_time": "2024-07-19T12:26:41.017302Z"
    }
   },
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Add messages essentially does this with more\n",
    "# robust handling\n",
    "# def add_messages(left: list, right: list):\n",
    "#     return left + right\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "21ac643b-cb06-4724-a80c-2862ba4773f1",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "We will first define the tools we want to use.\n",
    "For this simple example, we will use create a placeholder search engine.\n",
    "However, it is really easy to create your own tools - see documentation [here](https://python.langchain.com/v0.2/docs/how_to/custom_tools) on how to do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7ef57dd-5d6e-4ad3-9377-a92201c1310e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:44.260673Z",
     "start_time": "2024-07-19T12:26:44.254343Z"
    }
   },
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    return [\"The answer to your question lies within.\"]\n",
    "\n",
    "\n",
    "tools = [search]"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "01885785-b71a-44d1-b1d6-7b5b14d53b58",
   "metadata": {},
   "source": [
    "Now we can create our [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tool+node#toolnode). This \n",
    "object actually **runs** the tools (aka functions)  that the LLM has asked to use."
   ]
  },
  {
   "cell_type": "code",
   "id": "5cf3331e-ccb3-41c8-aeb9-a840a94d41e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:44.841186Z",
     "start_time": "2024-07-19T12:26:44.838943Z"
    }
   },
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "5497ed70-fce3-47f1-9cad-46f912bad6a5",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "Now we need to load the [chat model](https://python.langchain.com/v0.2/docs/concepts/#chat-models) to power our agent.\n",
    "For the design below, it must satisfy two criteria:\n",
    "\n",
    "1. It should work with **messages** (since our state contains a list of chat messages)\n",
    "2. It should work with [**tool calling**](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling).\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
    "    </p>\n",
    "</div>    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "id": "892b54b9-75f0-4804-9ed0-88b5e5532989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:58.841652Z",
     "start_time": "2024-07-19T12:26:58.819775Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# We will set streaming=True so that we can stream tokens\n",
    "# See the streaming section for more information on this.\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "a77995c0-bae2-4cee-a036-8688a90f05b9",
   "metadata": {},
   "source": [
    "\n",
    "After we've done this, we should make sure the model knows that it has these tools available to call.\n",
    "We can do this by converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd3cbae5-d92c-4559-a4aa-44721b80d107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:26:59.645907Z",
     "start_time": "2024-07-19T12:26:59.643420Z"
    }
   },
   "source": [
    "bound_model = model.bind_tools(tools)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "e03c5094-9297-4d19-a04e-3eedc75cefb4",
   "metadata": {},
   "source": [
    "## Define the graph \n",
    "\n",
    "We now need to define a few different nodes in our graph.\n",
    "In `langgraph`, a node can be either a function or a [runnable](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel).\n",
    "There are two main nodes we need for this:\n",
    "\n",
    "1. The agent: responsible for deciding what (if any) actions to take.\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
    "\n",
    "We will also need to define some edges.\n",
    "Some of these edges may be conditional.\n",
    "The reason they are conditional is that based on the output of a node, one of several paths may be taken.\n",
    "The path that is taken is not known until that node is run (the LLM decides).\n",
    "\n",
    "1. Conditional Edge: after the agent is called, we should either:\n",
    "   a. If the agent said to take an action, then the function to invoke tools should be called\n",
    "   b. If the agent said that it was finished, then it should finish\n",
    "2. Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next\n",
    "\n",
    "Let's define the nodes, as well as a function to decide how what conditional edge to take."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b541bb9-900c-40d0-964d-7b5dfee30667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:27:00.345371Z",
     "start_time": "2024-07-19T12:27:00.342960Z"
    }
   },
   "source": [
    "# Define the function that determines whether to continue or not\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"action\", \"__end__\"]:\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"__end__\"\n",
    "    # Otherwise if there is, we continue\n",
    "    return \"action\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: State):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "ffd6e892-946c-4899-8cc0-7c9291c1f73b",
   "metadata": {},
   "source": [
    "We can now put it all together and define the graph!"
   ]
  },
  {
   "cell_type": "code",
   "id": "812b4e70-4956-4415-8880-db48b3dcbad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:27:01.547251Z",
     "start_time": "2024-07-19T12:27:01.543517Z"
    }
   },
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "bc9c8536-f90b-44fa-958d-5df016c66d8f",
   "metadata": {},
   "source": [
    "**Persistence**\n",
    "\n",
    "To add in persistence, we pass in a checkpoint when compiling the graph"
   ]
  },
  {
   "cell_type": "code",
   "id": "6845ed6a-d155-4105-9160-28849877248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:27:02.930367Z",
     "start_time": "2024-07-19T12:27:02.928171Z"
    }
   },
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "79d29875-8aa8-434c-9f20-1c58346a6249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:27:03.470120Z",
     "start_time": "2024-07-19T12:27:03.466311Z"
    }
   },
   "source": [
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "7654ebcc-2179-41b4-92d1-6666f6f8634f",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        If you're using LangGraph Cloud, you <strong>don't need</strong> to pass checkpointer when compiling the graph, since it's done automatically.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d49697f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:27:05.379816Z",
     "start_time": "2024-07-19T12:27:04.739678Z"
    }
   },
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ],
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
   "metadata": {},
   "source": [
    "## Interacting with the Agent\n",
    "\n",
    "We can now interact with the agent and see that it remembers previous messages!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:27:08.200799Z",
     "start_time": "2024-07-19T12:27:06.901295Z"
    }
   },
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"hi! I'm Tam\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "hi! I'm Tam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hi Tam! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "08ae8246-11d5-40e1-8567-361e5bef8917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:28:36.118232Z",
     "start_time": "2024-07-19T12:28:35.396487Z"
    }
   },
   "source": [
    "input_message = HumanMessage(content=\"what is my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "what is my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Your name is Tam! How can I help you today?\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "3f47bbfc-d9ef-4288-ba4a-ebbc0136fa9d",
   "metadata": {},
   "source": [
    "If we want to start a new conversation, we can pass in a different thread id. Poof! All the memories are gone!"
   ]
  },
  {
   "cell_type": "code",
   "id": "273d56a8-f40f-4a51-a27f-7c6bb2bda0ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:29:35.291414Z",
     "start_time": "2024-07-19T12:29:34.049032Z"
    }
   },
   "source": [
    "input_message = HumanMessage(content=\"what is my name?\")\n",
    "for event in app.stream(\n",
    "    {\"messages\": [input_message]},\n",
    "    {\"configurable\": {\"thread_id\": \"3\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "what is my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. If you'd like to share your name or any other details, feel free to do so!\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "e833f994",
   "metadata": {},
   "source": [
    "All the checkpoints are persisted to the checkpointer, so you can always resume previous threads."
   ]
  },
  {
   "cell_type": "code",
   "id": "8578a66d-6489-4e03-8c23-fd0530278455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T12:30:22.048443Z",
     "start_time": "2024-07-19T12:30:21.152450Z"
    }
   },
   "source": [
    "input_message = HumanMessage(content=\"You forgot??\")\n",
    "for event in app.stream(\n",
    "    {\"messages\": [input_message]},\n",
    "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "You forgot??\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Not at all! Your name is Tam. If there's anything specific you'd like to discuss or ask, feel free to let me know!\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
