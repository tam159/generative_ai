{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Qwen",
   "id": "eba9d25a695811d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformers Usage",
   "id": "77bbbb7f05a64338"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T09:41:46.315688Z",
     "start_time": "2025-06-27T09:41:42.265026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Requires transformers>=4.51.0\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM"
   ],
   "id": "8d55128e3df4939f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-27T09:42:01.349758Z",
     "start_time": "2025-06-27T09:42:01.337324Z"
    }
   },
   "source": [
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T09:42:13.172907Z",
     "start_time": "2025-06-27T09:42:06.856016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "\n",
    "print(\"scores: \", scores)"
   ],
   "id": "c2b38b6b8135d868",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/may/tech/generative_ai/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.9994982481002808, 0.9993619322776794]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## vLLM Usage",
   "id": "ec611b870788b6f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:02:44.224277Z",
     "start_time": "2025-06-26T10:02:39.054714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Requires vllm>=0.8.5\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, is_torch_npu_available\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "import gc\n",
    "import math\n",
    "from vllm.inputs.data import TokensPrompt"
   ],
   "id": "70dd2f211d148b41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 17:02:42 [importing.py:17] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 06-26 17:02:42 [importing.py:29] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n",
      "INFO 06-26 17:02:43 [__init__.py:244] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:02:44.836223Z",
     "start_time": "2025-06-26T10:02:44.832337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ],
   "id": "792badd1b45ab8ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:02:45.205244Z",
     "start_time": "2025-06-26T10:02:45.193493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_instruction(instruction, query, doc):\n",
    "    text = [\n",
    "        {\"role\": \"system\", \"content\": \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\"},\n",
    "        {\"role\": \"user\", \"content\": f\"<Instruct>: {instruction}\\n\\n<Query>: {query}\\n\\n<Document>: {doc}\"}\n",
    "    ]\n",
    "    return text\n",
    "\n",
    "def process_inputs(pairs, instruction, max_length, suffix_tokens):\n",
    "    messages = [format_instruction(instruction, query, doc) for query, doc in pairs]\n",
    "    messages =  tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=False, enable_thinking=False\n",
    "    )\n",
    "    messages = [ele[:max_length] + suffix_tokens for ele in messages]\n",
    "    messages = [TokensPrompt(prompt_token_ids=ele) for ele in messages]\n",
    "    return messages\n",
    "\n",
    "def compute_logits(model, messages, sampling_params, true_token, false_token):\n",
    "    outputs = model.generate(messages, sampling_params, use_tqdm=False)\n",
    "    scores = []\n",
    "    for i in range(len(outputs)):\n",
    "        final_logits = outputs[i].outputs[0].logprobs[-1]\n",
    "        token_count = len(outputs[i].outputs[0].token_ids)\n",
    "        if true_token not in final_logits:\n",
    "            true_logit = -10\n",
    "        else:\n",
    "            true_logit = final_logits[true_token].logprob\n",
    "        if false_token not in final_logits:\n",
    "            false_logit = -10\n",
    "        else:\n",
    "            false_logit = final_logits[false_token].logprob\n",
    "        true_score = math.exp(true_logit)\n",
    "        false_score = math.exp(false_logit)\n",
    "        score = true_score / (true_score + false_score)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Reduce batch size and process sequentially\n",
    "def compute_logits_safe(model, messages, sampling_params, true_token, false_token):\n",
    "    scores = []\n",
    "    for message in messages:  # Process one at a time\n",
    "        try:\n",
    "            outputs = model.generate([message], sampling_params, use_tqdm=False)\n",
    "            # Your existing scoring logic for single output\n",
    "            final_logits = outputs[0].outputs[0].logprobs[-1]\n",
    "\n",
    "            true_logit = final_logits.get(true_token, type('obj', (object,), {'logprob': -10})).logprob\n",
    "            false_logit = final_logits.get(false_token, type('obj', (object,), {'logprob': -10})).logprob\n",
    "\n",
    "            true_score = math.exp(true_logit)\n",
    "            false_score = math.exp(false_logit)\n",
    "            score = true_score / (true_score + false_score)\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing single input: {e}\")\n",
    "            scores.append(0.5)  # Default neutral score\n",
    "    return scores\n"
   ],
   "id": "fe217b883f13995e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:02:45.655962Z",
     "start_time": "2025-06-26T10:02:45.651687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for GPU availability and set appropriate configuration\n",
    "number_of_gpu = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs detected: {number_of_gpu}\")"
   ],
   "id": "d56c2ba05ece707b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs detected: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:03:54.441389Z",
     "start_time": "2025-06-26T10:02:46.335493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure tensor_parallel_size is at least 1 and validate GPU availability\n",
    "if number_of_gpu == 0:\n",
    "    print(\"No GPUs detected. Running on CPU mode.\")\n",
    "    # For CPU execution, don't use tensor_parallel_size parameter\n",
    "    model = LLM(\n",
    "    model='Qwen/Qwen3-Reranker-0.6B',\n",
    "    max_model_len=10000,  # Reduced from 10000\n",
    "    # enable_prefix_caching=False,  # Disable for stability, need to be False or commented\n",
    "    # enforce_eager=True,  # Use eager execution\n",
    "    # max_num_seqs=1,  # Process one at a time\n",
    "    # gpu_memory_utilization=0.0,  # Force CPU\n",
    ")\n",
    "\n",
    "else:\n",
    "    print(f\"Using {number_of_gpu} GPU(s) for tensor parallelism.\")\n",
    "    model = LLM(\n",
    "        model='Qwen/Qwen3-Reranker-0.6B',\n",
    "        tensor_parallel_size=number_of_gpu,\n",
    "        max_model_len=10000,\n",
    "        enable_prefix_caching=True,\n",
    "        gpu_memory_utilization=0.8\n",
    "    )"
   ],
   "id": "55355a516c5c8610",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs detected. Running on CPU mode.\n",
      "INFO 06-26 17:02:53 [config.py:823] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 06-26 17:02:53 [config.py:3220] Your device 'cpu' doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 06-26 17:02:53 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-26 17:02:53 [arg_utils.py:1653] cpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 06-26 17:02:53 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\n",
      "WARNING 06-26 17:02:53 [cpu.py:135] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 06-26 17:02:53 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='Qwen/Qwen3-Reranker-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-Reranker-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-Reranker-0.6B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "WARNING 06-26 17:02:55 [cpu_worker.py:445] Auto thread-binding is not supported due to the lack of package numa and psutil,fallback to no thread-binding. To get better performance,please try to manually bind threads.\n",
      "INFO 06-26 17:02:55 [cpu.py:69] Using Torch SDPA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W626 17:03:00.193277000 ProcessGroupGloo.cpp:757] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 17:03:50 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-26 17:03:51 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 06-26 17:03:51 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aede2f59b7c1470e88a8605dc4405ee5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 17:03:53 [default_loader.py:272] Loading weights took 2.12 seconds\n",
      "INFO 06-26 17:03:53 [executor_base.py:113] # cpu blocks: 2340, # CPU blocks: 0\n",
      "INFO 06-26 17:03:53 [executor_base.py:118] Maximum concurrency for 10000 tokens per request: 3.74x\n",
      "INFO 06-26 17:03:54 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 0.70 seconds\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:05:05.026663Z",
     "start_time": "2025-06-26T10:05:02.879537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Reranker-0.6B')\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "max_length=8192\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "true_token = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
    "false_token = tokenizer(\"no\", add_special_tokens=False).input_ids[0]\n",
    "sampling_params = SamplingParams(temperature=0,\n",
    "    max_tokens=1,\n",
    "    logprobs=20,\n",
    "    allowed_token_ids=[true_token, false_token],\n",
    ")\n",
    "\n",
    "\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "pairs = list(zip(queries, documents))\n",
    "inputs = process_inputs(pairs, task, max_length-len(suffix_tokens), suffix_tokens)\n",
    "scores = compute_logits(model, inputs, sampling_params, true_token, false_token)\n",
    "print('scores', scores)\n",
    "\n",
    "destroy_model_parallel()"
   ],
   "id": "620cca695d10f5b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-26 17:05:05 [cpu.py:243] Pin memory is not supported on CPU.\n",
      "scores [0.997949256383843, 0.9992145261195076]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jina",
   "id": "ca21389f3b2bbd84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer",
   "id": "ce73bc6063e9a9d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:41:17.143950Z",
     "start_time": "2025-06-30T07:41:17.139221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification"
   ],
   "id": "4626ed5bb6302c03",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:41:17.635132Z",
     "start_time": "2025-06-30T07:41:17.629418Z"
    }
   },
   "cell_type": "code",
   "source": "torch.backends.mps.is_built()",
   "id": "98f72f4a565482c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:41:45.646752Z",
     "start_time": "2025-06-30T07:41:43.136298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'jinaai/jina-reranker-v2-base-multilingual',\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.to(\"mps\" if torch.backends.mps.is_built() else \"cpu\") # 'cuda', 'mps', or 'cpu' if no GPU is available\n",
    "model.eval()"
   ],
   "id": "a847a35d64303a1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "    )\n",
       "    (emb_drop): Dropout(p=0.1, inplace=False)\n",
       "    (emb_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (mixer): MHA(\n",
       "            (Wqkv): LinearResidual(in_features=768, out_features=2304, bias=True)\n",
       "            (inner_attn): SelfAttention(\n",
       "              (drop): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): CrossAttention(\n",
       "              (drop): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:41:56.680242Z",
     "start_time": "2025-06-30T07:41:55.705131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example query and documents\n",
    "query = \"Organic skincare products for sensitive skin\"\n",
    "documents = [\n",
    "    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n",
    "    \"New makeup trends focus on bold colors and innovative techniques\",\n",
    "    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n",
    "    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n",
    "    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n",
    "    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n",
    "    \"针对敏感肌专门设计的天然有机护肤产品\",\n",
    "    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n",
    "    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n",
    "    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n",
    "]\n",
    "\n",
    "# construct sentence pairs\n",
    "sentence_pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "scores = model.compute_score(sentence_pairs, max_length=1024)"
   ],
   "id": "e417f273566387b0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:41:58.339967Z",
     "start_time": "2025-06-30T07:41:58.336598Z"
    }
   },
   "cell_type": "code",
   "source": "print(scores)",
   "id": "e91cca2fe6e41bdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8300437331199646, 0.09534945338964462, 0.6306849718093872, 0.08269733935594559, 0.7620701193809509, 0.09947021305561066, 0.9263036847114563, 0.05921025201678276, 0.842863142490387, 0.1127953976392746]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:42:24.963025Z",
     "start_time": "2025-06-30T07:42:24.899427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = model.rerank(\n",
    "    query,\n",
    "    documents,\n",
    "    max_query_length=512,\n",
    "    max_length=1024,\n",
    "    top_n=3\n",
    ")"
   ],
   "id": "a0e252f1da22239",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:42:25.534155Z",
     "start_time": "2025-06-30T07:42:25.529102Z"
    }
   },
   "cell_type": "code",
   "source": "print(result)",
   "id": "bd740935b386155b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'document': '针对敏感肌专门设计的天然有机护肤产品', 'relevance_score': 0.9263036847114563, 'index': 6}, {'document': '敏感肌のために特別に設計された天然有機スキンケア製品', 'relevance_score': 0.842863142490387, 'index': 8}, {'document': 'Organic skincare for sensitive skin with aloe vera and chamomile.', 'relevance_score': 0.8300437331199646, 'index': 0}]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c6ff78332a962d99"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
