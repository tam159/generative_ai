{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf1de44-4047-46cf-a04c-dbf910d9e179",
   "metadata": {},
   "source": [
    "# BM25 Retriever\n",
    "In this guide, we define a bm25 retriever that search documents using bm25 method.\n",
    "\n",
    "This notebook is very similar to the RouterQueryEngine notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73fead-ec2c-4346-bd08-e183c13c7e29",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2630bc",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "id": "a60fea8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:41:02.453844Z",
     "start_time": "2024-07-04T08:41:02.451267Z"
    }
   },
   "source": [
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-retrievers-bm25"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9bc13275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:41:03.151480Z",
     "start_time": "2024-07-04T08:41:03.149712Z"
    }
   },
   "source": "# !pip install llama-index",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a2d59778-4cda-47b5-8cd0-b80fee91d1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:41:03.425143Z",
     "start_time": "2024-07-04T08:41:03.421588Z"
    }
   },
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "04941476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:41:04.277943Z",
     "start_time": "2024-07-04T08:41:03.793164Z"
    }
   },
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c628448c-573c-4eeb-a7e1-707fe8cc575c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:41:07.474866Z",
     "start_time": "2024-07-04T08:41:04.710295Z"
    }
   },
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 10 threads.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8d87b60f",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c9456c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:41:20.658497Z",
     "start_time": "2024-07-04T08:41:19.724486Z"
    }
   },
   "source": [
    "!mkdir -p ~/learning/generative_ai/llm/notebook/enhanced_retriever/data\n",
    "!curl https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt > ~/learning/generative_ai/llm/notebook/enhanced_retriever/data/paul_graham_essay.txt"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100 75042  100 75042    0     0   111k      0 --:--:-- --:--:-- --:--:--  111k\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "787174ed-10ce-47d7-82fd-9ca7f891eea7",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore."
   ]
  },
  {
   "cell_type": "code",
   "id": "1fc1b8ac-bf55-4d60-841c-61698663322f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:45.037683Z",
     "start_time": "2024-07-04T08:42:45.016238Z"
    }
   },
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"data/paul_graham_essay.txt\"]).load_data()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "7081194a-ede7-478e-bff2-23e89e23ef16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:46.104411Z",
     "start_time": "2024-07-04T08:42:45.881454Z"
    }
   },
   "source": [
    "# initialize LLM + node parser\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8f61bca2-c3b4-4ef0-a8f1-367933aa6d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:46.794798Z",
     "start_time": "2024-07-04T08:42:46.790817Z"
    }
   },
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "0d6162df-9da7-4aad-a2ca-eb318f67daec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:49.883801Z",
     "start_time": "2024-07-04T08:42:47.704253Z"
    }
   },
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "4fa4b18c-4f9b-4311-b1d9-9cbf24687e2c",
   "metadata": {},
   "source": [
    "## BM25 Retriever\n",
    "\n",
    "We will search document with bm25 retriever."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee3f7c3b-69b4-48d5-bf22-ac51a4e3179f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:50.957695Z",
     "start_time": "2024-07-04T08:42:50.877230Z"
    }
   },
   "source": [
    "# We can pass in the index, doctore, or list of nodes to create the retriever\n",
    "retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "7b8c4c12-1a30-425e-8312-04be050b2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:51.632072Z",
     "start_time": "2024-07-04T08:42:51.397955Z"
    }
   },
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# will retrieve context from specific companies\n",
    "nodes = retriever.retrieve(\"What happened at Viaweb and Interleaf?\")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** 57923292-a72a-49f6-92e7-1a40514cc4e4<br>**Similarity:** 1.284290506425639<br>**Text:** Notes\n\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** a079209a-9b18-4e21-adcf-6fdf97fe5db5<br>**Similarity:** 1.1042845548871<br>**Text:** I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking ...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "2749c34e-97c0-4bd5-8358-377a94b8b2d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:51.825042Z",
     "start_time": "2024-07-04T08:42:51.820945Z"
    }
   },
   "source": [
    "nodes = retriever.retrieve(\"What did Paul Graham do after RISD?\")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** 8bf31f1c-1533-45c4-a8a5-2d212fbef734<br>**Similarity:** 5.1818783699170625<br>**Text:** Now they are, though. Now you could continue using McCarthy's axiomatic approach till you'd defin...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** 19225f28-0307-4345-9e7d-3fd21f4a0efe<br>**Similarity:** 1.101230109848415<br>**Text:** Painting students were supposed to express themselves, which to the more worldly ones meant to tr...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "73460e63-3c7a-49f4-80d2-ea9f4033848e",
   "metadata": {},
   "source": [
    "## Router Retriever with bm25 method\n",
    "\n",
    "Now we will combine bm25 retriever with vector index retriever."
   ]
  },
  {
   "cell_type": "code",
   "id": "5ceedab8-c9e8-4ec9-be91-cba27482a7e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:53.198234Z",
     "start_time": "2024-07-04T08:42:53.189094Z"
    }
   },
   "source": [
    "from llama_index.core.tools import RetrieverTool\n",
    "\n",
    "vector_retriever = VectorIndexRetriever(index)\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)\n",
    "\n",
    "retriever_tools = [\n",
    "    RetrieverTool.from_defaults(\n",
    "        retriever=vector_retriever,\n",
    "        description=\"Useful in most cases\",\n",
    "    ),\n",
    "    RetrieverTool.from_defaults(\n",
    "        retriever=bm25_retriever,\n",
    "        description=\"Useful if searching about specific information\",\n",
    "    ),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "dcb850f9-c8b4-4843-b66c-8cb2a977c0e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:42:54.325884Z",
     "start_time": "2024-07-04T08:42:53.942389Z"
    }
   },
   "source": [
    "from llama_index.core.retrievers import RouterRetriever\n",
    "\n",
    "retriever = RouterRetriever.from_defaults(\n",
    "    retriever_tools=retriever_tools,\n",
    "    llm=llm,\n",
    "    select_multi=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "f3e29b3e-38e3-4f0b-a263-d267ed003cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:43:32.283817Z",
     "start_time": "2024-07-04T08:43:31.066710Z"
    }
   },
   "source": [
    "# will retrieve all context from the author's life\n",
    "nodes = retriever.retrieve(\n",
    "    \"Can you give me all the context regarding the author's life?\"\n",
    ")\n",
    "for node in nodes:\n",
    "    display_source_node(node)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Selecting retriever 1: The question asks for specific information about the author's life, making this choice the most relevant..\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for NodeWithScore\nnode\n  Can't instantiate abstract class BaseNode with abstract methods get_content, get_metadata_str, get_type, hash, set_content (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# will retrieve all context from the author's life\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m nodes \u001B[38;5;241m=\u001B[39m \u001B[43mretriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCan you give me all the context regarding the author\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ms life?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m nodes:\n\u001B[1;32m      6\u001B[0m     display_source_node(node)\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[1;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[1;32m    228\u001B[0m )\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/base/base_retriever.py:243\u001B[0m, in \u001B[0;36mBaseRetriever.retrieve\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mas_trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    240\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mRETRIEVE,\n\u001B[1;32m    241\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str},\n\u001B[1;32m    242\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n\u001B[0;32m--> 243\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    244\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001B[1;32m    245\u001B[0m         retrieve_event\u001B[38;5;241m.\u001B[39mon_end(\n\u001B[1;32m    246\u001B[0m             payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: nodes},\n\u001B[1;32m    247\u001B[0m         )\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[1;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[1;32m    228\u001B[0m )\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/retrievers/router_retriever.py:110\u001B[0m, in \u001B[0;36mRouterRetriever._retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    108\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to select retriever\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m     cur_results \u001B[38;5;241m=\u001B[39m \u001B[43mselected_retriever\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m     retrieved_results \u001B[38;5;241m=\u001B[39m {n\u001B[38;5;241m.\u001B[39mnode\u001B[38;5;241m.\u001B[39mnode_id: n \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m cur_results}\n\u001B[1;32m    113\u001B[0m query_event\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: retrieved_results\u001B[38;5;241m.\u001B[39mvalues()})\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[1;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[1;32m    228\u001B[0m )\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/base/base_retriever.py:243\u001B[0m, in \u001B[0;36mBaseRetriever.retrieve\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mas_trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    240\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mRETRIEVE,\n\u001B[1;32m    241\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str},\n\u001B[1;32m    242\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m retrieve_event:\n\u001B[0;32m--> 243\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    244\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001B[1;32m    245\u001B[0m         retrieve_event\u001B[38;5;241m.\u001B[39mon_end(\n\u001B[1;32m    246\u001B[0m             payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: nodes},\n\u001B[1;32m    247\u001B[0m         )\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[1;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[1;32m    228\u001B[0m )\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/llama_index/retrievers/bm25/base.py:92\u001B[0m, in \u001B[0;36mBM25Retriever._retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m     90\u001B[0m nodes: List[NodeWithScore] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ix \u001B[38;5;129;01min\u001B[39;00m top_n:\n\u001B[0;32m---> 92\u001B[0m     nodes\u001B[38;5;241m.\u001B[39mappend(\u001B[43mNodeWithScore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nodes\u001B[49m\u001B[43m[\u001B[49m\u001B[43mix\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mscores\u001B[49m\u001B[43m[\u001B[49m\u001B[43mix\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nodes\n",
      "File \u001B[0;32m~/.virtualenvs/generative_ai/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[0;34m(__pydantic_self__, **data)\u001B[0m\n\u001B[1;32m    339\u001B[0m values, fields_set, validation_error \u001B[38;5;241m=\u001B[39m validate_model(__pydantic_self__\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, data)\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_error:\n\u001B[0;32m--> 341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m validation_error\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    343\u001B[0m     object_setattr(__pydantic_self__, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m'\u001B[39m, values)\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for NodeWithScore\nnode\n  Can't instantiate abstract class BaseNode with abstract methods get_content, get_metadata_str, get_type, hash, set_content (type=type_error)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "b1b4b717",
   "metadata": {},
   "source": [
    "## Advanced - Hybrid Retriever + Re-Ranking\n",
    "\n",
    "Here we extend the base retriever class and create a custom retriever that always uses the vector retriever and BM25 retreiver.\n",
    "\n",
    "Then, nodes can be re-ranked and filtered. This lets us keep intermediate top-k values large and letting the re-ranking filter out un-needed nodes.\n",
    "\n",
    "To best demonstrate this, we will use a larger set of source documents -- Chapter 3 from the 2022 IPCC Climate Report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be4ec7",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ebd3ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:15:41.759031Z",
     "start_time": "2024-07-04T08:14:41.693705Z"
    }
   },
   "source": "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100 20.7M  100 20.7M    0     0   354k      0  0:00:59  0:00:59 --:--:--  389k 326k      0  0:01:05  0:00:17  0:00:48  358k  0:00:44  365k6 17.9M    0     0   350k      0  0:01:00  0:00:52  0:00:08  364k\r\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "3f7b5a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:43:40.170042Z",
     "start_time": "2024-07-04T08:43:40.168372Z"
    }
   },
   "source": [
    "# !pip install pypdf"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "48bd32ac-6d99-4bb5-8559-45b61d528525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:43:58.556538Z",
     "start_time": "2024-07-04T08:43:40.541443Z"
    }
   },
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "7c771a80-92e3-4178-b4a3-1f03c489b1a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:04.607004Z",
     "start_time": "2024-07-04T08:44:04.599227Z"
    }
   },
   "source": [
    "# initialize llm + node parser\n",
    "# -- here, we set a smaller chunk size, to allow for more effective re-ranking\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "# limit to a smaller section\n",
    "nodes = splitter.get_nodes_from_documents(\n",
    "    [Document(text=documents[0].get_content()[:1000000])]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "52f4243d-fd8f-4193-bf6a-73c07d661bcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:05.076215Z",
     "start_time": "2024-07-04T08:44:05.072778Z"
    }
   },
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "ccfc9879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:06.776502Z",
     "start_time": "2024-07-04T08:44:05.644829Z"
    }
   },
   "source": [
    "index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "0474be06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:07.420294Z",
     "start_time": "2024-07-04T08:44:07.405747Z"
    }
   },
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# retireve the top 10 most similar nodes using embeddings\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "# retireve the top 10 most similar nodes using bm25\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "99e603a1",
   "metadata": {},
   "source": [
    "### Custom Retriever Implementation"
   ]
  },
  {
   "cell_type": "code",
   "id": "917a3d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:08.471027Z",
     "start_time": "2024-07-04T08:44:08.468276Z"
    }
   },
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "9495e47c-2008-41ac-87e8-582d6d798190",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:09.350577Z",
     "start_time": "2024-07-04T08:44:09.348709Z"
    }
   },
   "source": [
    "index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "bfd3b287",
   "metadata": {},
   "source": [
    "### Re-Ranker Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1d910de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:10.706674Z",
     "start_time": "2024-07-04T08:44:10.705035Z"
    }
   },
   "source": "# !pip install sentence-transformers",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "638e7786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:45:46.640492Z",
     "start_time": "2024-07-04T08:45:44.783853Z"
    }
   },
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(top_n=4, model=\"BAAI/bge-reranker-base\")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "58a78eec",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "id": "9235f79d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:45:50.438717Z",
     "start_time": "2024-07-04T08:45:49.032574Z"
    }
   },
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "retrieved_nodes = hybrid_retriever.retrieve(\n",
    "    \"What is the impact of climate change on the ocean?\"\n",
    ")\n",
    "reranked_nodes = reranker.postprocess_nodes(\n",
    "    retrieved_nodes,\n",
    "    query_bundle=QueryBundle(\n",
    "        \"What is the impact of climate change on the ocean?\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Initial retrieval: \", len(retrieved_nodes), \" nodes\")\n",
    "print(\"Re-ranked retrieval: \", len(reranked_nodes), \" nodes\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial retrieval:  13  nodes\n",
      "Re-ranked retrieval:  4  nodes\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "5674f1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:45:51.749917Z",
     "start_time": "2024-07-04T08:45:51.745319Z"
    }
   },
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in reranked_nodes:\n",
    "    display_source_node(node)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** b921ca47-329e-45ee-95e4-03a16e753aff<br>**Similarity:** 0.0009219407220371068<br>**Text:** Ghebrehiwet, S.-I.  Ito, W.  Kiessling, P .  Martinetto, E.  Ojea, \nM.-F . Racault, B.  Rost, and...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** 1d2ed9ab-cd19-4f74-b862-c2dc2f20143a<br>**Similarity:** 0.0006790422485210001<br>**Text:** SPM379\n3\nOceans and Coastal \nEcosystems and Their Services\nThis chapter should be cited as:\nCoole...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** 92bf3c5a-71bb-4fba-9b92-02aa4e4b476a<br>**Similarity:** 0.0006047315546311438<br>**Text:** In: Climate \nChange 2022: Impacts, Adaptation and Vulnerability. Contribution of Working Group II...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Node ID:** f5ff880a-dac6-4b8f-95ad-08b40926dc35<br>**Similarity:** 0.0004132408066652715<br>**Text:** Helen \nGurney-Smith (Canada), Marjolijn Haasnoot (The Netherlands), Rebecca Harris (Australia), S...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "c24caa0d",
   "metadata": {},
   "source": [
    "### Full Query Engine "
   ]
  },
  {
   "cell_type": "code",
   "id": "549b9a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:45:56.305119Z",
     "start_time": "2024-07-04T08:45:52.691892Z"
    }
   },
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What is the impact of climate change on the ocean?\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 54.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "171668b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:45:56.310522Z",
     "start_time": "2024-07-04T08:45:56.306753Z"
    }
   },
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**`Final Response:`** Climate change significantly impacts oceans and coastal ecosystems, affecting their services. These impacts include rising sea temperatures, ocean acidification, and changes in marine biodiversity. These changes can disrupt marine food webs, alter fish populations, and affect the livelihoods of communities dependent on marine resources. Additionally, the increased frequency and intensity of extreme weather events can damage coastal habitats and infrastructure. Adaptation and mitigation strategies are essential to address these challenges and protect ocean and coastal ecosystems."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc36adf281c36924"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
